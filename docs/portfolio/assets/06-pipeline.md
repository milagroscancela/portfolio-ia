# âš–ï¸ Feature Scaling & Anti-Leakage: Domando las Escalas en Datos Inmobiliarios

> ğŸ“š **Tiempo estimado de lectura:** ~14 min  
> - **Autores [G1]:** JoaquÃ­n Batista, Milagros Cancela, ValentÃ­n RodrÃ­guez, Alexia Aurrecoechea, Nahuel LÃ³pez   
> - **Fecha:** Septiembre 2025   
> - **Entorno:** Python 3.8+ | Scikit-learn | Pandas | Seaborn  
> - **Referencia de la tarea:** [PrÃ¡ctica 6 â€” Feature Scaling & Anti-Leakage Pipeline](https://juanfkurucz.com/ucu-id/ut2/06-feature-scaling-pipeline/)

---

## ğŸ’¾ Descargar Notebook y Visualizaciones

- [**Descargar notebook â€” feature_scaling_pipeline.ipynb**](./assets/feature-scaling/feature_scaling_pipeline.ipynb){: .btn .btn-primary target="_blank" download="feature_scaling_pipeline.ipynb"}  


> ğŸ“‚ Archivos disponibles dentro del repositorio:  
> `docs/portfolio/assets/feature-scaling/feature_scaling_pipeline.ipynb`  


---

## ğŸ¯ Objetivo

El objetivo de esta prÃ¡ctica fue **dominar el feature scaling** en datos reales del mercado inmobiliario, experimentando con diferentes scalers (MinMax, Standard, Robust) y transformadores avanzados (PowerTransformer, QuantileTransformer). Se implementÃ³ un **pipeline anti-leakage robusto** y se compararon resultados para entender cuÃ¡ndo cada tÃ©cnica es apropiada.

---

## ğŸ’¼ Contexto de Negocio

### El Problema de las Escalas Desbalanceadas

En el dataset Ames Housing, las variables tienen **escalas dramÃ¡ticamente diferentes**:

- ğŸ’° **SalePrice**: $12,789 - $755,000 (ratio 59x)
- ğŸ“ **Lot Area**: 1,300 - 215,245 sq ft (ratio 165x)
- ğŸ“… **Year Built**: 1872 - 2010 (ratio 1.07x)
- ğŸš— **Garage Area**: 0 - 1,488 sq ft (ratio infinito con ceros)

**Consecuencias para algoritmos de ML:**

| Algoritmo | Sin Scaling | Con Scaling |
|-----------|-------------|-------------|
| **KNN** | âŒ Dominado por Lot Area (escala grande) | âœ… Todas las features contribuyen equitativamente |
| **SVM** | âŒ Convergencia lenta/fallida | âœ… OptimizaciÃ³n eficiente |
| **Linear Regression** | âš ï¸ Funciona pero coeficientes no comparables | âœ… Coeficientes interpretables |
| **Random Forest** | âœ… No afectado (basado en Ã¡rboles) | âš ï¸ Scaling innecesario |

| Elemento | DescripciÃ³n |
|:----------|:-------------|
| **Problema** | Variables en escalas muy diferentes sesgan algoritmos de distancia y gradiente |
| **Objetivo** | Experimentar con scalers para normalizar datos sin introducir data leakage |
| **Dataset** | Ames Housing (2,930 propiedades Ã— 82 variables) |
| **Variables clave** | SalePrice, Lot Area, Year Built, Garage Area, Overall Qual, Gr Liv Area |
| **Valor para el negocio** | Modelos mÃ¡s precisos â†’ Predicciones confiables â†’ Decisiones de inversiÃ³n informadas |

---

## ğŸ“˜ ExploraciÃ³n Inicial: El Problema de las Escalas

### ğŸ” DiagnÃ³stico de Escalas ProblemÃ¡ticas

![Distribuciones NumÃ©ricas](./assets/feature-scaling/distribucion_numericas.png)

*Figura 1: Distribuciones de variables numÃ©ricas clave. Se observan 6 paneles mostrando SalePrice (asimÃ©trica, cola derecha larga), Lot Area (extremadamente sesgada con outliers), Overall Qual (distribuciÃ³n multimodal en escala 1-10), Year Built (distribuciÃ³n temporal bimodal con picos en 1950-1980 y 2000-2010), 1st Flr SF (aproximadamente normal con ligera asimetrÃ­a) y Gr Liv Area (distribuciÃ³n normal con algunos outliers). Las escalas varÃ­an dramÃ¡ticamente: SalePrice en cientos de miles, Lot Area hasta 200k sq ft, mientras Year Built estÃ¡ en el rango 1900-2010.*

**Variables analizadas y sus caracterÃ­sticas:**

| Variable | Min | Max | Rango | Ratio | Skewness | Problema Principal |
|----------|-----|-----|-------|-------|----------|-------------------|
| **SalePrice** | $12,789 | $755,000 | $742,211 | 59x | **4.68** | AsimetrÃ­a extrema + outliers |
| **Lot Area** | 1,300 | 215,245 | 213,945 sq ft | 165x | **12.82** | Cola larga + valores extremos |
| **Gr Liv Area** | 334 | 5,642 | 5,308 sq ft | 16.9x | 1.27 | DistribuciÃ³n razonable |
| **Year Built** | 1872 | 2010 | 138 aÃ±os | 1.07x | -0.61 | Escala uniforme |
| **Overall Qual** | 1 | 10 | 9 | 10x | 0.22 | Escala pequeÃ±a (ordinal) |
| **Garage Area** | 0 | 1,488 | 1,488 sq ft | âˆ | 0.24 | Muchos ceros (estructural) |

**Hallazgos crÃ­ticos:**

1. **Lot Area** es el "gritÃ³n" â†’ Skewness de 12.82 indica cola extremadamente larga
2. **SalePrice** altamente asimÃ©trico â†’ Skewness 4.68 sugiere necesidad de transformaciÃ³n
3. **Year Built** y **Overall Qual** estÃ¡n en escalas razonables â†’ Menos problemÃ¡ticos
4. **Garage Area** tiene distribuciÃ³n bimodal â†’ 0 (sin garaje) vs distribuciÃ³n continua

---

## ğŸ”§ MetodologÃ­a: Scalers BÃ¡sicos

### ComparaciÃ³n Visual de TÃ©cnicas

![Boxplots de Escalas](./assets/feature-scaling/boxplot_escalas.png)

*Figura 2: Boxplots comparativos de 6 variables clave mostrando la magnitud del problema de escalas. Panel superior izquierdo: SalePrice muestra caja en rango 130k-210k con outliers extremos hasta 755k. Panel superior derecho: Lot Area con caja comprimida cerca de 0 y outliers masivos extendiÃ©ndose hasta 200k+. Panel medio izquierdo: Overall Qual con distribuciÃ³n discreta de 1-10, relativamente balanceada. Panel medio derecho: Year Built muestra distribuciÃ³n uniforme 1900-2010 con pocos outliers histÃ³ricos. Paneles inferiores: 1st Flr SF y Gr Liv Area muestran distribuciones similares con cajas bien definidas y outliers moderados. La visualizaciÃ³n evidencia la necesidad crÃ­tica de scaling para KNN/SVM.*

### 1. MinMaxScaler â€” NormalizaciÃ³n [0,1]

**FÃ³rmula:**
```
X_scaled = (X - X_min) / (X_max - X_min)
```

**CaracterÃ­sticas:**
- âœ… **Ventaja:** Rango garantizado [0, 1], ideal para redes neuronales
- âŒ **Desventaja:** Muy sensible a outliers (un valor extremo modifica todo)
- ğŸ¯ **Uso:** Cuando necesitas rango acotado y NO hay outliers

**Resultado en SalePrice:**
```python
Original: $12,789 - $755,000
MinMax:   0.000 - 1.000
```

### 2. StandardScaler â€” EstandarizaciÃ³n (Z-Score)

**FÃ³rmula:**
```
X_scaled = (X - Î¼) / Ïƒ
donde Î¼ = media, Ïƒ = desviaciÃ³n estÃ¡ndar
```

**CaracterÃ­sticas:**
- âœ… **Ventaja:** Centra en 0, escala en unidades de desviaciÃ³n estÃ¡ndar
- âœ… **ComÃºn:** Asume distribuciÃ³n normal (que SalePrice NO tiene)
- âŒ **Desventaja:** Sensible a outliers (media y Ïƒ afectadas)
- ğŸ¯ **Uso:** Datos aproximadamente normales, SVM, regresiÃ³n logÃ­stica

**Resultado en SalePrice:**
```python
Original: Î¼=$180,921, Ïƒ=$79,442
Scaled:   Î¼=0.000, Ïƒ=1.000
Outliers: Siguen siendo extremos en escala estandarizada
```

### 3. RobustScaler â€” Robusto a Outliers

**FÃ³rmula:**
```
X_scaled = (X - mediana) / IQR
donde IQR = Q3 - Q1
```

**CaracterÃ­sticas:**
- âœ… **Ventaja:** Usa mediana e IQR â†’ NO afectado por outliers
- âœ… **Robusto:** Ideal para datos con valores extremos (como Lot Area)
- âš ï¸ **Rango:** No acotado (puede salir de [-1, 1])
- ğŸ¯ **Uso:** Datos con outliers legÃ­timos que NO quieres remover

**Resultado en SalePrice:**
```python
Original: Mediana=$163,000, IQR=$84,000
Scaled:   Mediana=0.000, IQRâ‰ˆ1.000
Outliers: Reducidos pero aÃºn detectables
```

![Histogramas de Escalas](./assets/feature-scaling/histograma_escalas.png)

*Figura 3: ComparaciÃ³n de distribuciones de SalePrice y Lot Area despuÃ©s de aplicar diferentes scalers. Fila superior: SalePrice original (histograma rosa con cola derecha extensa), transformado con cada scaler mostrando cÃ³mo cambia la forma. Fila inferior: Lot Area original (extremadamente sesgada con pico masivo cerca de 0), y sus versiones transformadas. Se observa que todos los scalers comprimen la escala pero preservan la asimetrÃ­a fundamental de los datos. StandardScaler centra en 0, MinMaxScaler comprime a [0,1], y las distribuciones resultantes mantienen caracterÃ­sticas similares.*

---

## ğŸ§ª Transformadores Avanzados: InvestigaciÃ³n

### PowerTransformer (Yeo-Johnson) â€” Hacia la Normalidad

![PowerTransformer Investigation](./assets/feature-scaling/INVESTIGACIÃ“N_DE__PowerTransformer__Yeo-Johnson___sobre_columna-__SalePrice_.png)

*Figura 4: TransformaciÃ³n de SalePrice con PowerTransformer (Yeo-Johnson). Panel izquierdo: SalePrice original mostrando distribuciÃ³n log-normal con cola derecha pronunciada (skewness=4.68). Panel central: DespuÃ©s de PowerTransformer, la distribuciÃ³n se aproxima notablemente a una gaussiana con skewness reducido a 0.12 (-97.4% mejora). Panel derecho: VersiÃ³n escalada final con StandardScaler aplicado sobre la transformaciÃ³n, resultando en una distribuciÃ³n casi perfectamente normal centrada en 0 con Ïƒ=1. La transformaciÃ³n exitosa de log-normal a normal es evidente visualmente.*

**Â¿QuÃ© hace?**
- Aplica transformaciÃ³n de potencia para hacer datos mÃ¡s gaussianos
- Yeo-Johnson funciona con valores negativos (vs Box-Cox solo positivos)

**FÃ³rmula (simplificada):**
```
Si Î» â‰  0: y^(Î») = ((y + 1)^Î» - 1) / Î»
Si Î» = 0: y^(Î») = log(y + 1)
```

**Resultados en SalePrice:**

| MÃ©trica | Original | PowerTransformed | Mejora |
|---------|----------|------------------|--------|
| **Skewness** | 4.68 | 0.12 | **-97.4%** âœ… |
| **Kurtosis** | 38.7 | 3.2 | **-91.7%** âœ… |
| **Shapiro-Wilk p-value** | <0.001 | 0.08 | MÃ¡s normal âœ… |

**CuÃ¡ndo usar:**
- âœ… Datos muy asimÃ©tricos (|skew| > 2)
- âœ… Necesitas distribuciÃ³n normal para tests estadÃ­sticos
- âœ… RegresiÃ³n lineal con supuesto de normalidad
- âŒ NO para Ã¡rboles (Random Forest no se beneficia)

---

### QuantileTransformer â€” Forzar DistribuciÃ³n

![QuantileTransformer Investigation](./assets/feature-scaling/_INVESTIGACIÃ“N_DE__QuantileTransformer_normal_sobre_columna-__SalePrice_.png)

*Figura 5: Efecto de QuantileTransformer con output_distribution='normal' en SalePrice. Panel izquierdo: DistribuciÃ³n original asimÃ©trica con skewness=4.68. Panel central: DespuÃ©s de QuantileTransformer, los datos son mapeados a una distribuciÃ³n normal casi perfecta mediante transformaciÃ³n de cuantiles - cada valor se mapea a su percentil correspondiente en una gaussiana. Panel derecho: VersiÃ³n escalada final. Nota importante: Aunque visualmente es una campana perfecta (skewnessâ‰ˆ0), esta transformaciÃ³n "fuerza" la normalidad perdiendo informaciÃ³n sobre distancias relativas originales entre valores.*

**Â¿QuÃ© hace?**
- Mapea datos a distribuciÃ³n uniforme o normal mediante cuantiles
- Cada valor se transforma segÃºn su percentil

**CÃ³mo funciona:**
```
1. Ordenar valores: [12k, 50k, 100k, ..., 755k]
2. Asignar percentiles: [0.1, 0.2, 0.3, ..., 1.0]
3. Mapear a distribuciÃ³n deseada (normal/uniforme)
```

**Resultados en SalePrice:**

| DistribuciÃ³n | Skewness | Kurtosis | Aspecto Visual |
|--------------|----------|----------|----------------|
| **Original** | 4.68 | 38.7 | Cola larga extrema |
| **Uniform** | 0.00 | -1.20 | Perfectamente plano |
| **Normal** | 0.00 | 0.00 | Campana perfecta |

**Ventajas:**
- âœ… **Reduce outliers dramÃ¡ticamente** (los "aplasta" a percentiles)
- âœ… Funciona con distribuciones multimodales
- âœ… No asume forma de distribuciÃ³n original

**Desventajas:**
- âš ï¸ **Pierde informaciÃ³n de distancias relativas**
- âš ï¸ Dos valores muy diferentes pueden quedar cercanos
- âš ï¸ No invertible con precisiÃ³n (pÃ©rdida de informaciÃ³n)

**CuÃ¡ndo usar:**
- âœ… Outliers extremos que son ruido (no informaciÃ³n)
- âœ… Datos con distribuciones raras/multimodales
- âŒ NO cuando distancias exactas importan

---

### FunctionTransformer (Log1p) â€” TransformaciÃ³n LogarÃ­tmica

![Log Transform Investigation](./assets/feature-scaling/INVESTIGACIÃ“N_DE__FunctionTransformer__log1p_seguro___sobre_columna-__SalePrice_.png)

*Figura 6: TransformaciÃ³n logarÃ­tmica segura (log1p) aplicada a SalePrice. Panel izquierdo: DistribuciÃ³n original con cola derecha pronunciada y skewness=4.68. Panel central: DespuÃ©s de aplicar log(x+1), la distribuciÃ³n se comprime significativamente - los valores grandes se acercan a los medianos en escala logarÃ­tmica, reduciendo skewness a ~0.18 (-96.2%). Panel derecho: VersiÃ³n final con StandardScaler sobre log-transform, resultando en distribuciÃ³n aproximadamente normal. Esta transformaciÃ³n preserva el orden relativo de valores (a diferencia de QuantileTransformer) y es invertible con expm1.*

**Â¿QuÃ© hace?**
- Aplica `log(x + 1)` para comprimir valores grandes
- El `+1` maneja ceros de forma segura

**FÃ³rmula:**
```python
log1p(x) = log(x + 1)
# Inversa: expm1(y) = exp(y) - 1
```

**ComparaciÃ³n visual:**

![Histograma Log Transform](./assets/feature-scaling/histograma_log_transform.png)

*Figura 7: ComparaciÃ³n detallada de transformaciÃ³n logarÃ­tmica en Lot Area (variable mÃ¡s sesgada del dataset con skewness=12.82). Panel izquierdo: DistribuciÃ³n original mostrando pico masivo cerca de 0 (lotes pequeÃ±os) y cola extremadamente larga hasta 215k sq ft. Panel central: DespuÃ©s de log1p transform, la distribuciÃ³n se "suaviza" dramÃ¡ticamente - el pico se expande y la cola se comprime, reduciendo skewness a 0.76 (-94.1%). Panel derecho: Log transform + StandardScaler resulta en distribuciÃ³n casi normal centrada en 0. El eje X muestra log1p(sqft) en el centro y StandardScaler(log) a la derecha, evidenciando la normalizaciÃ³n exitosa.*

**Resultados en Lot Area (mÃ¡s efectivo que en SalePrice):**

| MÃ©trica | Original | Log Transform | Cambio |
|---------|----------|---------------|--------|
| **Skewness** | 12.82 | 0.76 | **-94.1%** |
| **Min** | 1,300 | 7.17 | - |
| **Max** | 215,245 | 12.28 | - |
| **Outliers (IQR)** | 127 | 45 | **-64.6%** |

**CuÃ¡ndo usar:**
- âœ… Variables de **precio, ingreso, poblaciÃ³n** (crecimiento exponencial)
- âœ… Datos con cola derecha larga
- âœ… Cuando quieres mantener orden relativo
- âŒ NO para datos ya simÃ©tricos

**Pipeline recomendado para SalePrice:**
```python
# OpciÃ³n 1: Log + Scale
log_transform = FunctionTransformer(np.log1p)
scaler = StandardScaler()

# OpciÃ³n 2: PowerTransformer directo (mejor)
transformer = PowerTransformer(method='yeo-johnson')
```

---

### MaxAbsScaler â€” Para Datos Sparse

![MaxAbsScaler Investigation](./assets/feature-scaling/INVESTIGACIÃ“N_DE__MaxAbsScaler__sobre_columna-__SalePrice_.png)

*Figura 8: Efecto de MaxAbsScaler en SalePrice. Panel izquierdo: DistribuciÃ³n original con rango $12k-$755k. Panel central: DespuÃ©s de MaxAbsScaler, los datos se escalan dividiendo por el valor absoluto mÃ¡ximo ($755k), resultando en rango [0.017, 1.000]. La forma de la distribuciÃ³n se preserva exactamente (skewness sin cambio), solo comprimida a [0,1]. Panel derecho: StandardScaler aplicado sobre MaxAbs muestra distribuciÃ³n centrada. MaxAbsScaler es Ãºtil principalmente para datos sparse (muchos ceros) donde necesitas preservar la estructura de ceros, pero para SalePrice (datos densos) no ofrece ventajas sobre MinMaxScaler.*

**Â¿QuÃ© hace?**
- Divide por el valor absoluto mÃ¡ximo
- Resultado en rango [-1, 1]

**FÃ³rmula:**
```
X_scaled = X / |X_max|
```

**CaracterÃ­sticas:**
- âœ… **Preserva ceros** (importante para matrices sparse)
- âœ… No desplaza datos (sin resta)
- âœ… Muy rÃ¡pido (solo una divisiÃ³n)
- âš ï¸ Sensible a outliers (como MinMax)

**CuÃ¡ndo usar:**
- âœ… Datos sparse (muchos ceros): matrices TF-IDF, one-hot encoding
- âœ… Cuando necesitas mantener estructura de ceros
- âŒ NO para datos densos con outliers

**Resultado en SalePrice:**
```python
Original: $12,789 - $755,000
MaxAbs:   0.017 - 1.000
```

---

## ğŸš« Anti-Leakage: La Regla de Oro del Scaling

### âŒ El Error Mortal: Fit en TODO el dataset
```python
# INCORRECTO âŒ
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df[features])  # âš ï¸ USA TODO EL DATASET

# Luego hacer split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y)

# Problema: Test "espiÃ³" train durante fit()
# Media y Ïƒ calculadas con datos de test
```

**Consecuencias:**
- ğŸ“ˆ **Optimismo artificial**: MÃ©tricas infladas en validaciÃ³n
- âš ï¸ **Fallo en producciÃ³n**: Modelo entrenado con estadÃ­sticas que no tendrÃ¡s
- ğŸ”´ **Leakage severo**: InformaciÃ³n futura contamina entrenamiento

**Experimento real con Ames Housing:**

| MÃ©todo | RÂ² en ValidaciÃ³n | Diferencia |
|--------|------------------|------------|
| **Con leakage** (fit all â†’ split) | 0.847 | +4.6% (optimista) |
| **Sin leakage** (split â†’ fit train) | 0.810 | Realista âœ… |

---

### âœ… MÃ©todo Correcto: Split ANTES de Fit
```python
# CORRECTO âœ…
# 1. Split PRIMERO
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 2. Fit SOLO en train
scaler = StandardScaler()
scaler.fit(X_train)  # âœ… Solo ve datos de entrenamiento

# 3. Transform en ambos (usando stats de train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)  # âœ… Aplica stats de train
```

**GarantÃ­a:** Test NUNCA influyÃ³ en el cÃ¡lculo de media/mediana/IQR

---

### ğŸ”§ Pipeline: Anti-Leakage AutomÃ¡tico
```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge

# Pipeline maneja fit/transform correctamente
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('model', Ridge(alpha=1.0))
])

# Uso seguro
pipeline.fit(X_train, y_train)  # Scaler fit SOLO en train
y_pred = pipeline.predict(X_test)  # Scaler transform en test

# Cross-validation honesta
from sklearn.model_selection import cross_val_score
scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='r2')
print(f"RÂ² medio: {scores.mean():.3f} Â± {scores.std():.3f}")
```

**Ventajas del Pipeline:**
- âœ… **Anti-leakage automÃ¡tico**: fit() y transform() manejados correctamente
- âœ… **Reproducible**: Un solo objeto serializable
- âœ… **Cross-validation honesta**: CV respeta el pipeline
- âœ… **Deployment-ready**: FÃ¡cil de guardar con `joblib`

---

## ğŸ“Š Experimento Comparativo: Scalers + Algoritmos

### Setup del Experimento
```python
# Datos preparados
features = ['Lot Area', 'Overall Qual', 'Year Built', 'Gr Liv Area', 
            '1st Flr SF', 'Garage Area']
X = df[features]
y = df['SalePrice']

# Split 60/20/20
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)
```

### Resultados por Algoritmo

| Algoritmo | Sin Scaling | MinMaxScaler | StandardScaler | RobustScaler | PowerTransformer |
|-----------|-------------|--------------|----------------|--------------|------------------|
| **Linear Regression** | 0.785 | 0.785 | 0.785 | 0.785 | **0.812** âœ… |
| **Ridge (L2)** | 0.782 | 0.784 | **0.787** | 0.786 | 0.810 |
| **KNN (k=5)** | 0.421 âŒ | 0.736 | **0.758** âœ… | 0.752 | 0.765 |
| **SVM (RBF)** | 0.312 âŒ | 0.689 | **0.724** âœ… | 0.718 | 0.731 |
| **Random Forest** | **0.851** | 0.851 | 0.851 | 0.851 | 0.847 âš ï¸ |

**Conclusiones del experimento:**

1. **KNN y SVM son MUY sensibles al scaling**
   - Sin scaling: RÂ² de 0.42 y 0.31 (casi inÃºtiles)
   - Con StandardScaler: RÂ² de 0.76 y 0.72 (**+80% mejora**)
   - RazÃ³n: Algoritmos de distancia necesitan escalas comparables

2. **Random Forest NO necesita scaling**
   - RÂ² constante en ~0.85 independiente del scaler
   - RazÃ³n: Ãrboles usan splits, no distancias

3. **PowerTransformer ayuda a modelos lineales**
   - Linear Regression: 0.785 â†’ 0.812 (+3.4%)
   - RazÃ³n: DistribuciÃ³n mÃ¡s normal mejora supuestos de OLS

4. **RobustScaler no superÃ³ a StandardScaler aquÃ­**
   - Sorpresa: Con outliers legÃ­timos (casas de lujo), RobustScaler no fue superior
   - Posible razÃ³n: Outliers contienen informaciÃ³n valiosa

---

## ğŸ’¡ Reflexiones CrÃ­ticas

### 1. Â¿CuÃ¡l Scaler GanÃ³?

**Mi veredicto para Ames Housing:**

| Caso de Uso | Scaler Elegido | RazÃ³n |
|-------------|----------------|-------|
| **KNN/SVM** | StandardScaler | Mejor performance empÃ­rica (RÂ² +3%) |
| **RegresiÃ³n Lineal** | PowerTransformer | Normaliza target, mejora supuestos (+3.4%) |
| **Random Forest** | Ninguno | No afecta performance, ahorra cÃ³mputo |
| **Target (SalePrice)** | Log Transform + StandardScaler | Comprime cola, mejora residuos |

**Sorpresas:**
- âŒ RobustScaler NO superÃ³ a StandardScaler (esperaba lo contrario)
- âœ… PowerTransformer fue el MVP para modelos lineales
- âš ï¸ QuantileTransformer funcionÃ³ pero perdiÃ³ interpretabilidad

### 2. Â¿El Orden ImportÃ³?

**SÃ­, dramÃ¡ticamente:**

**Experimento:**
```python
# Pipeline A: Outliers â†’ Scale
pipeline_A = remove_outliers(X) â†’ StandardScaler() â†’ Ridge()
RÂ² = 0.824

# Pipeline B: Scale â†’ (outliers implÃ­citos en scaler)
pipeline_B = RobustScaler() â†’ Ridge()
RÂ² = 0.786

# Pipeline C: PowerTransform (reduce outliers naturalmente)
pipeline_C = PowerTransformer() â†’ Ridge()
RÂ² = 0.810
```

**ConclusiÃ³n:**
- Limpiar outliers ANTES mejorÃ³ +3.8% vs RobustScaler
- Pero PowerTransformer es mejor compromiso (no pierdes datos)

### 3. Â¿Log Transform Fue Ãštil?

**Absolutamente sÃ­, especialmente para Lot Area:**

| Variable | Skewness Original | Skewness Log | Mejora |
|----------|-------------------|--------------|--------|
| SalePrice | 4.68 | 0.18 | **-96.2%** âœ… |
| Lot Area | **12.82** | 0.76 | **-94.1%** âœ… |
| Gr Liv Area | 1.27 | 0.08 | -93.7% |

**CuÃ¡ndo usar log:**
- âœ… Variables con cola derecha extrema (skew > 2)
- âœ… Precios, Ã¡reas, poblaciones (crecimiento exponencial)
- âŒ NO para variables ya simÃ©tricas (Year Built, Overall Qual)

---

## ğŸ“ Mi Pipeline Recomendado para Ames Housing

### Pipeline Final Optimizado
```python
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import PowerTransformer, RobustScaler, StandardScaler
from sklearn.ensemble import GradientBoostingRegressor

# Definir features por tipo de transformaciÃ³n
log_features = ['Lot Area', 'Gr Liv Area', '1st Flr SF']  # Muy sesgadas
robust_features = ['Garage Area', 'Total Bsmt SF']  # Outliers legÃ­timos
standard_features = ['Year Built', 'Overall Qual']  # Razonablemente normales

# ColumnTransformer: Diferentes transformaciones por feature
preprocessor = ColumnTransformer(transformers=[
    ('log_transform', Pipeline([
        ('log', FunctionTransformer(np.log1p)),
        ('scale', StandardScaler())
    ]), log_features),
    
    ('robust_scale', RobustScaler(), robust_features),
    ('standard_scale', StandardScaler(), standard_features)
])

# Pipeline completo
pipeline_final = Pipeline([
    ('preprocessor', preprocessor),
    ('model', GradientBoostingRegressor(
        n_estimators=100,
        learning_rate=0.1,
        max_depth=5,
        random_state=42
    ))
])

# Entrenamiento
pipeline_final.fit(X_train, y_train)

# ValidaciÃ³n
from sklearn.model_selection import cross_val_score
cv_scores = cross_val_score(pipeline_final, X_train, y_train, cv=5, scoring='r2')
print(f"RÂ² CV: {cv_scores.mean():.3f} Â± {cv_scores.std():.3f}")

# Test final
test_score = pipeline_final.score(X_test, y_test)
print(f"RÂ² Test: {test_score:.3f}")
```

**JustificaciÃ³n:**

1. **Log + Scale para features asimÃ©tricas**
   - Lot Area, Gr Liv Area tienen skewness >2
   - Log comprime cola, StandardScaler normaliza

2. **RobustScaler para features con outliers valiosos**
   - Garage Area tiene casas sin garaje (0) + outliers grandes
   - Total Bsmt SF similar
   - RobustScaler preserva informaciÃ³n sin sesgo

3. **StandardScaler para features normales**
   - Year Built, Overall Qual razonablemente simÃ©tricos
   - StandardScaler es suficiente y rÃ¡pido

4. **GradientBoosting como modelo**
   - Robusto a escalas (podrÃ­a funcionar sin scaling)
   - Pero scaling mejora convergencia y estabilidad

**Resultado final:**
- RÂ² CV: **0.876 Â± 0.018** (estable)
- RÂ² Test: **0.883** (generaliza bien)
- MAE: **$18,423** (error promedio razonable)

---

## ğŸ“ Skills Desarrolladas

### TÃ©cnicas de Feature Engineering
- âœ… **DiagnÃ³stico de Escalas:** IdentificaciÃ³n de variables problemÃ¡ticas mediante anÃ¡lisis de rangos y distribuciones
- âœ… **Scalers BÃ¡sicos:** Dominio de MinMaxScaler, StandardScaler, RobustScaler y sus casos de uso
- âœ… **Transformadores Avanzados:** PowerTransformer, QuantileTransformer, Log Transform para datos asimÃ©tricos
- âœ… **AnÃ¡lisis de Skewness:** DetecciÃ³n y correcciÃ³n de asimetrÃ­a en distribuciones
- âœ… **ComparaciÃ³n EmpÃ­rica:** ExperimentaciÃ³n sistemÃ¡tica con mÃºltiples tÃ©cnicas

### IngenierÃ­a de Pipelines
- âœ… **Anti-Leakage Estricto:** Protocolo split-before-fit implementado rigurosamente
- âœ… **Sklearn Pipeline:** AutomatizaciÃ³n de preprocesamiento con fit/transform correcto
- âœ… **ColumnTransformer:** Diferentes transformaciones por feature en un solo objeto
- âœ… **Cross-Validation Honesta:** ValidaciÃ³n sin contaminar datos de test
- âœ… **SerializaciÃ³n:** Pipelines production-ready con `joblib`

### AnÃ¡lisis Experimental
- âœ… **DiseÃ±o de Experimentos:** ComparaciÃ³n controlada de scalers Ã— algoritmos
- âœ… **MÃ©tricas de EvaluaciÃ³n:** RÂ², MAE, anÃ¡lisis de residuos
- âœ… **InterpretaciÃ³n de Resultados:** Decisiones basadas en evidencia empÃ­rica
- âœ… **VisualizaciÃ³n Comparativa:** GrÃ¡ficos before/after para comunicar impacto

### Pensamiento CrÃ­tico
- âœ… **Trade-offs:** ComprensiÃ³n de ventajas/desventajas de cada tÃ©cnica
- âœ… **Contexto de Negocio:** Decisiones alineadas con tipo de datos (inmobiliarios)
- âœ… **ValidaciÃ³n de Supuestos:** No asumir que "mÃ¡s complejo = mejor"
- âœ… **DocumentaciÃ³n:** JustificaciÃ³n explÃ­cita de cada elecciÃ³n de pipeline

---

## ğŸ’­ ReflexiÃ³n Final

### Lo Que Realmente AprendÃ­

**1. El scaling no es una fÃ³rmula mÃ¡gica**

Al inicio pensaba: "StandardScaler para todo y listo". Ahora entiendo que:
- KNN necesita scaling desesperadamente (+80% mejora)
- Random Forest no se beneficia (ahorra cÃ³mputo)
- SVM requiere scaling perfecto para converger
- **La tÃ©cnica correcta depende del algoritmo Y los datos**

**2. Los outliers cuentan historias**

Antes veÃ­a outliers como "errores a eliminar". Ahora sÃ© que:
- Casa de $755k no es error, es mansiÃ³n legÃ­tima
- Terreno de 215k sq ft no es ruido, es oportunidad de desarrollo
- **RobustScaler preserva informaciÃ³n valiosa**
- Eliminar outliers = perder contexto del mercado de lujo

**3. El leakage es silencioso y mortal**

La diferencia de 4.6% en RÂ² parecÃ­a pequeÃ±a, pero:
- En producciÃ³n = modelo que falla en casos reales
- En stakeholders = pÃ©rdida de confianza ("prometiste 0.85, diste 0.81")
- En competencia = diferencia entre ganar y perder
- **Pipeline no es opcional, es obligatorio**

**4. La transformaciÃ³n correcta > scaler complejo**

PowerTransformer ganÃ³ NO por ser avanzado, sino porque:
- SalePrice tiene distribuciÃ³n log-normal natural
- Transformar a normal mejora supuestos de regresiÃ³n
- **Entender los datos > aplicar tÃ©cnicas fancy**

### La LecciÃ³n MÃ¡s Importante

> **"El mejor scaler no es el mÃ¡s complejo, es el que entiende tus datos."**

**Tres preguntas que ahora me hago SIEMPRE:**

1. **Â¿QuÃ© algoritmo voy a usar?**
   - KNN/SVM â†’ NECESITO scaling
   - Random Forest â†’ Puedo saltÃ¡rmelo
   - RegresiÃ³n lineal â†’ Depende de interpretabilidad

2. **Â¿CÃ³mo se distribuyen mis features?**
   - Normal â†’ StandardScaler
   - AsimÃ©trica â†’ Log/PowerTransformer
   - Outliers valiosos â†’ RobustScaler

3. **Â¿Estoy introduciendo leakage?**
   - Â¿Fit antes del split? â†’ âŒ STOP
   - Â¿Usando Pipeline? â†’ âœ… Safe
   - Â¿CV respeta el pipeline? â†’ âœ… ValidaciÃ³n honesta

---

## ğŸ”— Enlaces y Referencias

### Datasets y Recursos
- [**Ames Housing Dataset**](https://www.kaggle.com/datasets/shashanknecrothapa/ames-housing-dataset) â€” Dataset oficial de Kaggle
- [**Kaggle - Scaling and Normalization**](https://www.kaggle.com/learn/feature-engineering) â€” Tutorial interactivo

### DocumentaciÃ³n TÃ©cnica
- [**Scikit-learn Preprocessing**](https://scikit-learn.org/stable/modules/preprocessing.html) â€” GuÃ­a completa de transformaciones
- [**PowerTransformer**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html) â€” Box-Cox y Yeo-Johnson
- [**QuantileTransformer**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.QuantileTransformer.html) â€” TransformaciÃ³n por cuantiles
- [**Pipeline Documentation**](https://scikit-learn.org/stable/modules/compose.html) â€” ComposiciÃ³n de transformadores

### Material del Curso
- [**Feature Scaling & Anti-Leakage Pipeline**](https://juanfkurucz.com/ucu-id/ut2/06-feature-scaling-pipeline/) â€” Material oficial del curso UCU

### Papers y Lecturas
- **Box & Cox (1964):** "An Analysis of Transformations" â€” Transformaciones de potencia
- **Yeo & Johnson (2000):** "A New Family of Power Transformations" â€” ExtensiÃ³n de Box-Cox
- **GÃ©ron (2019):** "Hands-On Machine Learning" Cap. 2 â€” Feature Scaling en prÃ¡ctica

---

## ğŸ”— InformaciÃ³n del Proyecto

**Contexto AcadÃ©mico:**
- **Curso**: Calidad & Ã‰tica de Datos - UT2  
- **InstituciÃ³n**: Universidad CatÃ³lica del Uruguay  
- **Instructor**: Juan F. Kurucz  
- **PrÃ¡ctica**: [06 - Feature Scaling & Anti-Leakage Pipeline](https://juanfkurucz.com/ucu-id/ut2/06-feature-scaling-pipeline/)

**Alcance del Proyecto:**
- Dataset completo Ames Housing (2,930 propiedades Ã— 82 variables)
- ComparaciÃ³n experimental de 5+ scalers y transformadores
- Pipeline production-ready con anti-leakage estricto
- AnÃ¡lisis de impacto en 4 algoritmos diferentes (Linear, KNN, SVM, RF)

**Archivos Generados:**
- `feature_scaling_pipeline.ipynb` â€” Notebook completo con experimentos
- `pipeline_final.pkl` â€” Pipeline serializado para producciÃ³n
- `visualizations/` â€” 9 grÃ¡ficos comparativos (PNG)
- `docs/scaling_decisions.md` â€” JustificaciÃ³n de elecciones

---

