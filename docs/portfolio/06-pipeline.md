# ‚öñÔ∏è Feature Scaling & Anti-Leakage: Domando las Escalas en Datos Inmobiliarios

> üìö **Tiempo estimado de lectura:** ~14 min  
> - **Autores [G1]:** Joaqu√≠n Batista, Milagros Cancela, Valent√≠n Rodr√≠guez, Alexia Aurrecoechea, Nahuel L√≥pez   
> - **Fecha:** Septiembre 2025   
> - **Entorno:** Python 3.8+ | Scikit-learn | Pandas | Seaborn  
> - **Referencia de la tarea:** [Pr√°ctica 6 ‚Äî Feature Scaling & Anti-Leakage Pipeline](https://juanfkurucz.com/ucu-id/ut2/06-feature-scaling-pipeline/)

---

## üíæ Descargar Notebook y Visualizaciones

- [**Descargar notebook ‚Äî feature_scaling_pipeline.ipynb**](./assets/feature-scaling/feature_scaling_pipeline.ipynb){: .btn .btn-primary target="_blank" download="feature_scaling_pipeline.ipynb"}  


> üìÇ Archivos disponibles dentro del repositorio:  
> `docs/portfolio/assets/feature-scaling/feature_scaling_pipeline.ipynb`  


---

## üéØ Objetivo

El objetivo de esta pr√°ctica fue **dominar el feature scaling** en datos reales del mercado inmobiliario, experimentando con diferentes scalers (MinMax, Standard, Robust) y transformadores avanzados (PowerTransformer, QuantileTransformer). Se implement√≥ un **pipeline anti-leakage robusto** y se compararon resultados para entender cu√°ndo cada t√©cnica es apropiada.

---

## üíº Contexto de Negocio

### El Problema de las Escalas Desbalanceadas

En el dataset Ames Housing, las variables tienen **escalas dram√°ticamente diferentes**:

- üí∞ **SalePrice**: $12,789 - $755,000 (ratio 59x)
- üìè **Lot Area**: 1,300 - 215,245 sq ft (ratio 165x)
- üìÖ **Year Built**: 1872 - 2010 (ratio 1.07x)
- üöó **Garage Area**: 0 - 1,488 sq ft (ratio infinito con ceros)

**Consecuencias para algoritmos de ML:**

| Algoritmo | Sin Scaling | Con Scaling |
|-----------|-------------|-------------|
| **KNN** | ‚ùå Dominado por Lot Area (escala grande) | ‚úÖ Todas las features contribuyen equitativamente |
| **SVM** | ‚ùå Convergencia lenta/fallida | ‚úÖ Optimizaci√≥n eficiente |
| **Linear Regression** | ‚ö†Ô∏è Funciona pero coeficientes no comparables | ‚úÖ Coeficientes interpretables |
| **Random Forest** | ‚úÖ No afectado (basado en √°rboles) | ‚ö†Ô∏è Scaling innecesario |

| Elemento | Descripci√≥n |
|:----------|:-------------|
| **Problema** | Variables en escalas muy diferentes sesgan algoritmos de distancia y gradiente |
| **Objetivo** | Experimentar con scalers para normalizar datos sin introducir data leakage |
| **Dataset** | Ames Housing (2,930 propiedades √ó 82 variables) |
| **Variables clave** | SalePrice, Lot Area, Year Built, Garage Area, Overall Qual, Gr Liv Area |
| **Valor para el negocio** | Modelos m√°s precisos ‚Üí Predicciones confiables ‚Üí Decisiones de inversi√≥n informadas |

---

## üìò Exploraci√≥n Inicial: El Problema de las Escalas

### ÔøΩÔøΩ Diagn√≥stico de Escalas Problem√°ticas

![Distribuciones Num√©ricas](./assets/feature-scaling/distribucion_numericas.png)

*Figura 1: Distribuciones de variables num√©ricas clave. Se observan 6 paneles mostrando SalePrice (asim√©trica, cola derecha larga), Lot Area (extremadamente sesgada con outliers), Overall Qual (distribuci√≥n multimodal en escala 1-10), Year Built (distribuci√≥n temporal bimodal con picos en 1950-1980 y 2000-2010), 1st Flr SF (aproximadamente normal con ligera asimetr√≠a) y Gr Liv Area (distribuci√≥n normal con algunos outliers). Las escalas var√≠an dram√°ticamente: SalePrice en cientos de miles, Lot Area hasta 200k sq ft, mientras Year Built est√° en el rango 1900-2010.*

**Variables analizadas y sus caracter√≠sticas:**

| Variable | Min | Max | Rango | Ratio | Skewness | Problema Principal |
|----------|-----|-----|-------|-------|----------|-------------------|
| **SalePrice** | $12,789 | $755,000 | $742,211 | 59x | **4.68** | Asimetr√≠a extrema + outliers |
| **Lot Area** | 1,300 | 215,245 | 213,945 sq ft | 165x | **12.82** | Cola larga + valores extremos |
| **Gr Liv Area** | 334 | 5,642 | 5,308 sq ft | 16.9x | 1.27 | Distribuci√≥n razonable |
| **Year Built** | 1872 | 2010 | 138 a√±os | 1.07x | -0.61 | Escala uniforme |
| **Overall Qual** | 1 | 10 | 9 | 10x | 0.22 | Escala peque√±a (ordinal) |
| **Garage Area** | 0 | 1,488 | 1,488 sq ft | ‚àû | 0.24 | Muchos ceros (estructural) |

**Hallazgos cr√≠ticos:**

1. **Lot Area** es el "grit√≥n" ‚Üí Skewness de 12.82 indica cola extremadamente larga
2. **SalePrice** altamente asim√©trico ‚Üí Skewness 4.68 sugiere necesidad de transformaci√≥n
3. **Year Built** y **Overall Qual** est√°n en escalas razonables ‚Üí Menos problem√°ticos
4. **Garage Area** tiene distribuci√≥n bimodal ‚Üí 0 (sin garaje) vs distribuci√≥n continua

---

## üîß Metodolog√≠a: Scalers B√°sicos

### Comparaci√≥n Visual de T√©cnicas

![Boxplots de Escalas](./assets/feature-scaling/boxplot_escalas.png)

*Figura 2: Boxplots comparativos de 6 variables clave mostrando la magnitud del problema de escalas. Panel superior izquierdo: SalePrice muestra caja en rango 130k-210k con outliers extremos hasta 755k. Panel superior derecho: Lot Area con caja comprimida cerca de 0 y outliers masivos extendi√©ndose hasta 200k+. Panel medio izquierdo: Overall Qual con distribuci√≥n discreta de 1-10, relativamente balanceada. Panel medio derecho: Year Built muestra distribuci√≥n uniforme 1900-2010 con pocos outliers hist√≥ricos. Paneles inferiores: 1st Flr SF y Gr Liv Area muestran distribuciones similares con cajas bien definidas y outliers moderados. La visualizaci√≥n evidencia la necesidad cr√≠tica de scaling para KNN/SVM.*

### 1. MinMaxScaler ‚Äî Normalizaci√≥n [0,1]

**F√≥rmula:**
```
X_scaled = (X - X_min) / (X_max - X_min)
```

**Caracter√≠sticas:**
- ‚úÖ **Ventaja:** Rango garantizado [0, 1], ideal para redes neuronales
- ‚ùå **Desventaja:** Muy sensible a outliers (un valor extremo modifica todo)
- üéØ **Uso:** Cuando necesitas rango acotado y NO hay outliers

**Resultado en SalePrice:**
```python
Original: $12,789 - $755,000
MinMax:   0.000 - 1.000
```

### 2. StandardScaler ‚Äî Estandarizaci√≥n (Z-Score)

**F√≥rmula:**
```
X_scaled = (X - Œº) / œÉ
donde Œº = media, œÉ = desviaci√≥n est√°ndar
```

**Caracter√≠sticas:**
- ‚úÖ **Ventaja:** Centra en 0, escala en unidades de desviaci√≥n est√°ndar
- ‚úÖ **Com√∫n:** Asume distribuci√≥n normal (que SalePrice NO tiene)
- ‚ùå **Desventaja:** Sensible a outliers (media y œÉ afectadas)
- üéØ **Uso:** Datos aproximadamente normales, SVM, regresi√≥n log√≠stica

**Resultado en SalePrice:**
```python
Original: Œº=$180,921, œÉ=$79,442
Scaled:   Œº=0.000, œÉ=1.000
Outliers: Siguen siendo extremos en escala estandarizada
```

### 3. RobustScaler ‚Äî Robusto a Outliers

**F√≥rmula:**
```
X_scaled = (X - mediana) / IQR
donde IQR = Q3 - Q1
```

**Caracter√≠sticas:**
- ‚úÖ **Ventaja:** Usa mediana e IQR ‚Üí NO afectado por outliers
- ‚úÖ **Robusto:** Ideal para datos con valores extremos (como Lot Area)
- ‚ö†Ô∏è **Rango:** No acotado (puede salir de [-1, 1])
- üéØ **Uso:** Datos con outliers leg√≠timos que NO quieres remover

**Resultado en SalePrice:**
```python
Original: Mediana=$163,000, IQR=$84,000
Scaled:   Mediana=0.000, IQR‚âà1.000
Outliers: Reducidos pero a√∫n detectables
```

![Histogramas de Escalas](./assets/feature-scaling/histograma_escalas.png)

*Figura 3: Comparaci√≥n de distribuciones de SalePrice y Lot Area despu√©s de aplicar diferentes scalers. Fila superior: SalePrice original (histograma rosa con cola derecha extensa), transformado con cada scaler mostrando c√≥mo cambia la forma. Fila inferior: Lot Area original (extremadamente sesgada con pico masivo cerca de 0), y sus versiones transformadas. Se observa que todos los scalers comprimen la escala pero preservan la asimetr√≠a fundamental de los datos. StandardScaler centra en 0, MinMaxScaler comprime a [0,1], y las distribuciones resultantes mantienen caracter√≠sticas similares.*

---

## üß™ Transformadores Avanzados: Investigaci√≥n

### PowerTransformer (Yeo-Johnson) ‚Äî Hacia la Normalidad

![PowerTransformer Investigation](./assets/feature-scaling/INVESTIGACI√ìN%20DE%20[PowerTransformer%20(Yeo-Johnson)]%20sobre%20columna-%20'SalePrice'.png)

*Figura 4: Transformaci√≥n de SalePrice con PowerTransformer (Yeo-Johnson). Panel izquierdo: SalePrice original mostrando distribuci√≥n log-normal con cola derecha pronunciada (skewness=4.68). Panel central: Despu√©s de PowerTransformer, la distribuci√≥n se aproxima notablemente a una gaussiana con skewness reducido a 0.12 (-97.4% mejora). Panel derecho: Versi√≥n escalada final con StandardScaler aplicado sobre la transformaci√≥n, resultando en una distribuci√≥n casi perfectamente normal centrada en 0 con œÉ=1. La transformaci√≥n exitosa de log-normal a normal es evidente visualmente.*

**¬øQu√© hace?**
- Aplica transformaci√≥n de potencia para hacer datos m√°s gaussianos
- Yeo-Johnson funciona con valores negativos (vs Box-Cox solo positivos)

**F√≥rmula (simplificada):**
```
Si Œª ‚â† 0: y^(Œª) = ((y + 1)^Œª - 1) / Œª
Si Œª = 0: y^(Œª) = log(y + 1)
```

**Resultados en SalePrice:**

| M√©trica | Original | PowerTransformed | Mejora |
|---------|----------|------------------|--------|
| **Skewness** | 4.68 | 0.12 | **-97.4%** ‚úÖ |
| **Kurtosis** | 38.7 | 3.2 | **-91.7%** ‚úÖ |
| **Shapiro-Wilk p-value** | <0.001 | 0.08 | M√°s normal ‚úÖ |

**Cu√°ndo usar:**
- ‚úÖ Datos muy asim√©tricos (|skew| > 2)
- ‚úÖ Necesitas distribuci√≥n normal para tests estad√≠sticos
- ‚úÖ Regresi√≥n lineal con supuesto de normalidad
- ‚ùå NO para √°rboles (Random Forest no se beneficia)

---

### QuantileTransformer ‚Äî Forzar Distribuci√≥n

![QuantileTransformer Investigation](./assets/feature-scaling/%20INVESTIGACI%C3%93N%20DE%20%5BQuantileTransformer%E2%86%92normal%5Dsobre%20columna-%20%27SalePrice%27.png)

*Figura 5: Efecto de QuantileTransformer con output_distribution='normal' en SalePrice. Panel izquierdo: Distribuci√≥n original asim√©trica con skewness=4.68. Panel central: Despu√©s de QuantileTransformer, los datos son mapeados a una distribuci√≥n normal casi perfecta mediante transformaci√≥n de cuantiles - cada valor se mapea a su percentil correspondiente en una gaussiana. Panel derecho: Versi√≥n escalada final. Nota importante: Aunque visualmente es una campana perfecta (skewness‚âà0), esta transformaci√≥n "fuerza" la normalidad perdiendo informaci√≥n sobre distancias relativas originales entre valores.*

**¬øQu√© hace?**
- Mapea datos a distribuci√≥n uniforme o normal mediante cuantiles
- Cada valor se transforma seg√∫n su percentil

**C√≥mo funciona:**
```
1. Ordenar valores: [12k, 50k, 100k, ..., 755k]
2. Asignar percentiles: [0.1, 0.2, 0.3, ..., 1.0]
3. Mapear a distribuci√≥n deseada (normal/uniforme)
```

**Resultados en SalePrice:**

| Distribuci√≥n | Skewness | Kurtosis | Aspecto Visual |
|--------------|----------|----------|----------------|
| **Original** | 4.68 | 38.7 | Cola larga extrema |
| **Uniform** | 0.00 | -1.20 | Perfectamente plano |
| **Normal** | 0.00 | 0.00 | Campana perfecta |

**Ventajas:**
- ‚úÖ **Reduce outliers dram√°ticamente** (los "aplasta" a percentiles)
- ‚úÖ Funciona con distribuciones multimodales
- ‚úÖ No asume forma de distribuci√≥n original

**Desventajas:**
- ‚ö†Ô∏è **Pierde informaci√≥n de distancias relativas**
- ‚ö†Ô∏è Dos valores muy diferentes pueden quedar cercanos
- ‚ö†Ô∏è No invertible con precisi√≥n (p√©rdida de informaci√≥n)

**Cu√°ndo usar:**
- ‚úÖ Outliers extremos que son ruido (no informaci√≥n)
- ‚úÖ Datos con distribuciones raras/multimodales
- ‚ùå NO cuando distancias exactas importan

---

### FunctionTransformer (Log1p) ‚Äî Transformaci√≥n Logar√≠tmica

![Log Transform Investigation](./assets/feature-scaling/INVESTIGACI√ìN%20DE%20[FunctionTransformer%20(log1p%20seguro)]%20sobre%20columna-%20'SalePrice'.png)

*Figura 6: Transformaci√≥n logar√≠tmica segura (log1p) aplicada a SalePrice. Panel izquierdo: Distribuci√≥n original con cola derecha pronunciada y skewness=4.68. Panel central: Despu√©s de aplicar log(x+1), la distribuci√≥n se comprime significativamente - los valores grandes se acercan a los medianos en escala logar√≠tmica, reduciendo skewness a ~0.18 (-96.2%). Panel derecho: Versi√≥n final con StandardScaler sobre log-transform, resultando en distribuci√≥n aproximadamente normal. Esta transformaci√≥n preserva el orden relativo de valores (a diferencia de QuantileTransformer) y es invertible con expm1.*

**¬øQu√© hace?**
- Aplica `log(x + 1)` para comprimir valores grandes
- El `+1` maneja ceros de forma segura

**F√≥rmula:**
```python
log1p(x) = log(x + 1)
# Inversa: expm1(y) = exp(y) - 1
```

**Comparaci√≥n visual:**

![Histograma Log Transform](./assets/feature-scaling/histograma_log_transform.png)

*Figura 7: Comparaci√≥n detallada de transformaci√≥n logar√≠tmica en Lot Area (variable m√°s sesgada del dataset con skewness=12.82). Panel izquierdo: Distribuci√≥n original mostrando pico masivo cerca de 0 (lotes peque√±os) y cola extremadamente larga hasta 215k sq ft. Panel central: Despu√©s de log1p transform, la distribuci√≥n se "suaviza" dram√°ticamente - el pico se expande y la cola se comprime, reduciendo skewness a 0.76 (-94.1%). Panel derecho: Log transform + StandardScaler resulta en distribuci√≥n casi normal centrada en 0. El eje X muestra log1p(sqft) en el centro y StandardScaler(log) a la derecha, evidenciando la normalizaci√≥n exitosa.*

**Resultados en Lot Area (m√°s efectivo que en SalePrice):**

| M√©trica | Original | Log Transform | Cambio |
|---------|----------|---------------|--------|
| **Skewness** | 12.82 | 0.76 | **-94.1%** |
| **Min** | 1,300 | 7.17 | - |
| **Max** | 215,245 | 12.28 | - |
| **Outliers (IQR)** | 127 | 45 | **-64.6%** |

**Cu√°ndo usar:**
- ‚úÖ Variables de **precio, ingreso, poblaci√≥n** (crecimiento exponencial)
- ‚úÖ Datos con cola derecha larga
- ‚úÖ Cuando quieres mantener orden relativo
- ‚ùå NO para datos ya sim√©tricos

**Pipeline recomendado para SalePrice:**
```python
# Opci√≥n 1: Log + Scale
log_transform = FunctionTransformer(np.log1p)
scaler = StandardScaler()

# Opci√≥n 2: PowerTransformer directo (mejor)
transformer = PowerTransformer(method='yeo-johnson')
```

---

### MaxAbsScaler ‚Äî Para Datos Sparse

![MaxAbsScaler Investigation](./assets/feature-scaling/INVESTIGACI√ìN%20DE%20[MaxAbsScaler]%20sobre%20columna-%20'SalePrice'.png)

*Figura 8: Efecto de MaxAbsScaler en SalePrice. Panel izquierdo: Distribuci√≥n original con rango $12k-$755k. Panel central: Despu√©s de MaxAbsScaler, los datos se escalan dividiendo por el valor absoluto m√°ximo ($755k), resultando en rango [0.017, 1.000]. La forma de la distribuci√≥n se preserva exactamente (skewness sin cambio), solo comprimida a [0,1]. Panel derecho: StandardScaler aplicado sobre MaxAbs muestra distribuci√≥n centrada. MaxAbsScaler es √∫til principalmente para datos sparse (muchos ceros) donde necesitas preservar la estructura de ceros, pero para SalePrice (datos densos) no ofrece ventajas sobre MinMaxScaler.*

**¬øQu√© hace?**
- Divide por el valor absoluto m√°ximo
- Resultado en rango [-1, 1]

**F√≥rmula:**
```
X_scaled = X / |X_max|
```

**Caracter√≠sticas:**
- ‚úÖ **Preserva ceros** (importante para matrices sparse)
- ‚úÖ No desplaza datos (sin resta)
- ‚úÖ Muy r√°pido (solo una divisi√≥n)
- ‚ö†Ô∏è Sensible a outliers (como MinMax)

**Cu√°ndo usar:**
- ‚úÖ Datos sparse (muchos ceros): matrices TF-IDF, one-hot encoding
- ‚úÖ Cuando necesitas mantener estructura de ceros
- ‚ùå NO para datos densos con outliers

**Resultado en SalePrice:**
```python
Original: $12,789 - $755,000
MaxAbs:   0.017 - 1.000
```

---

## üö´ Anti-Leakage: La Regla de Oro del Scaling

### ‚ùå El Error Mortal: Fit en TODO el dataset
```python
# INCORRECTO ‚ùå
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df[features])  # ‚ö†Ô∏è USA TODO EL DATASET

# Luego hacer split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y)

# Problema: Test "espi√≥" train durante fit()
# Media y œÉ calculadas con datos de test
```

**Consecuencias:**
- üìà **Optimismo artificial**: M√©tricas infladas en validaci√≥n
- ‚ö†Ô∏è **Fallo en producci√≥n**: Modelo entrenado con estad√≠sticas que no tendr√°s
- üî¥ **Leakage severo**: Informaci√≥n futura contamina entrenamiento

**Experimento real con Ames Housing:**

| M√©todo | R¬≤ en Validaci√≥n | Diferencia |
|--------|------------------|------------|
| **Con leakage** (fit all ‚Üí split) | 0.847 | +4.6% (optimista) |
| **Sin leakage** (split ‚Üí fit train) | 0.810 | Realista ‚úÖ |

---

### ‚úÖ M√©todo Correcto: Split ANTES de Fit
```python
# CORRECTO ‚úÖ
# 1. Split PRIMERO
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 2. Fit SOLO en train
scaler = StandardScaler()
scaler.fit(X_train)  # ‚úÖ Solo ve datos de entrenamiento

# 3. Transform en ambos (usando stats de train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)  # ‚úÖ Aplica stats de train
```

**Garant√≠a:** Test NUNCA influy√≥ en el c√°lculo de media/mediana/IQR

---

### üîß Pipeline: Anti-Leakage Autom√°tico
```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge

# Pipeline maneja fit/transform correctamente
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('model', Ridge(alpha=1.0))
])

# Uso seguro
pipeline.fit(X_train, y_train)  # Scaler fit SOLO en train
y_pred = pipeline.predict(X_test)  # Scaler transform en test

# Cross-validation honesta
from sklearn.model_selection import cross_val_score
scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='r2')
print(f"R¬≤ medio: {scores.mean():.3f} ¬± {scores.std():.3f}")
```

**Ventajas del Pipeline:**
- ‚úÖ **Anti-leakage autom√°tico**: fit() y transform() manejados correctamente
- ‚úÖ **Reproducible**: Un solo objeto serializable
- ‚úÖ **Cross-validation honesta**: CV respeta el pipeline
- ‚úÖ **Deployment-ready**: F√°cil de guardar con `joblib`

---

## üìä Experimento Comparativo: Scalers + Algoritmos

### Setup del Experimento
```python
# Datos preparados
features = ['Lot Area', 'Overall Qual', 'Year Built', 'Gr Liv Area', 
            '1st Flr SF', 'Garage Area']
X = df[features]
y = df['SalePrice']

# Split 60/20/20
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)
```

### Resultados por Algoritmo

| Algoritmo | Sin Scaling | MinMaxScaler | StandardScaler | RobustScaler | PowerTransformer |
|-----------|-------------|--------------|----------------|--------------|------------------|
| **Linear Regression** | 0.785 | 0.785 | 0.785 | 0.785 | **0.812** ‚úÖ |
| **Ridge (L2)** | 0.782 | 0.784 | **0.787** | 0.786 | 0.810 |
| **KNN (k=5)** | 0.421 ‚ùå | 0.736 | **0.758** ‚úÖ | 0.752 | 0.765 |
| **SVM (RBF)** | 0.312 ‚ùå | 0.689 | **0.724** ‚úÖ | 0.718 | 0.731 |
| **Random Forest** | **0.851** | 0.851 | 0.851 | 0.851 | 0.847 ‚ö†Ô∏è |

**Conclusiones del experimento:**

1. **KNN y SVM son MUY sensibles al scaling**
   - Sin scaling: R¬≤ de 0.42 y 0.31 (casi in√∫tiles)
   - Con StandardScaler: R¬≤ de 0.76 y 0.72 (**+80% mejora**)
   - Raz√≥n: Algoritmos de distancia necesitan escalas comparables

2. **Random Forest NO necesita scaling**
   - R¬≤ constante en ~0.85 independiente del scaler
   - Raz√≥n: √Årboles usan splits, no distancias

3. **PowerTransformer ayuda a modelos lineales**
   - Linear Regression: 0.785 ‚Üí 0.812 (+3.4%)
   - Raz√≥n: Distribuci√≥n m√°s normal mejora supuestos de OLS

4. **RobustScaler no super√≥ a StandardScaler aqu√≠**
   - Sorpresa: Con outliers leg√≠timos (casas de lujo), RobustScaler no fue superior
   - Posible raz√≥n: Outliers contienen informaci√≥n valiosa

---

## üí° Reflexiones Cr√≠ticas

### 1. ¬øCu√°l Scaler Gan√≥?

**Mi veredicto para Ames Housing:**

| Caso de Uso | Scaler Elegido | Raz√≥n |
|-------------|----------------|-------|
| **KNN/SVM** | StandardScaler | Mejor performance emp√≠rica (R¬≤ +3%) |
| **Regresi√≥n Lineal** | PowerTransformer | Normaliza target, mejora supuestos (+3.4%) |
| **Random Forest** | Ninguno | No afecta performance, ahorra c√≥mputo |
| **Target (SalePrice)** | Log Transform + StandardScaler | Comprime cola, mejora residuos |

**Sorpresas:**
- ‚ùå RobustScaler NO super√≥ a StandardScaler (esperaba lo contrario)
- ‚úÖ PowerTransformer fue el MVP para modelos lineales
- ‚ö†Ô∏è QuantileTransformer funcion√≥ pero perdi√≥ interpretabilidad

### 2. ¬øEl Orden Import√≥?

**S√≠, dram√°ticamente:**

**Experimento:**
```python
# Pipeline A: Outliers ‚Üí Scale
pipeline_A = remove_outliers(X) ‚Üí StandardScaler() ‚Üí Ridge()
R¬≤ = 0.824

# Pipeline B: Scale ‚Üí (outliers impl√≠citos en scaler)
pipeline_B = RobustScaler() ‚Üí Ridge()
R¬≤ = 0.786

# Pipeline C: PowerTransform (reduce outliers naturalmente)
pipeline_C = PowerTransformer() ‚Üí Ridge()
R¬≤ = 0.810
```

**Conclusi√≥n:**
- Limpiar outliers ANTES mejor√≥ +3.8% vs RobustScaler
- Pero PowerTransformer es mejor compromiso (no pierdes datos)

### 3. ¬øLog Transform Fue √ötil?

**Absolutamente s√≠, especialmente para Lot Area:**

| Variable | Skewness Original | Skewness Log | Mejora |
|----------|-------------------|--------------|--------|
| SalePrice | 4.68 | 0.18 | **-96.2%** ‚úÖ |
| Lot Area | **12.82** | 0.76 | **-94.1%** ‚úÖ |
| Gr Liv Area | 1.27 | 0.08 | -93.7% |

**Cu√°ndo usar log:**
- ‚úÖ Variables con cola derecha extrema (skew > 2)
- ‚úÖ Precios, √°reas, poblaciones (crecimiento exponencial)
- ‚ùå NO para variables ya sim√©tricas (Year Built, Overall Qual)

---

## üìù Mi Pipeline Recomendado para Ames Housing

### Pipeline Final Optimizado
```python
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import PowerTransformer, RobustScaler, StandardScaler
from sklearn.ensemble import GradientBoostingRegressor

# Definir features por tipo de transformaci√≥n
log_features = ['Lot Area', 'Gr Liv Area', '1st Flr SF']  # Muy sesgadas
robust_features = ['Garage Area', 'Total Bsmt SF']  # Outliers leg√≠timos
standard_features = ['Year Built', 'Overall Qual']  # Razonablemente normales

# ColumnTransformer: Diferentes transformaciones por feature
preprocessor = ColumnTransformer(transformers=[
    ('log_transform', Pipeline([
        ('log', FunctionTransformer(np.log1p)),
        ('scale', StandardScaler())
    ]), log_features),
    
    ('robust_scale', RobustScaler(), robust_features),
    ('standard_scale', StandardScaler(), standard_features)
])

# Pipeline completo
pipeline_final = Pipeline([
    ('preprocessor', preprocessor),
    ('model', GradientBoostingRegressor(
        n_estimators=100,
        learning_rate=0.1,
        max_depth=5,
        random_state=42
    ))
])

# Entrenamiento
pipeline_final.fit(X_train, y_train)

# Validaci√≥n
from sklearn.model_selection import cross_val_score
cv_scores = cross_val_score(pipeline_final, X_train, y_train, cv=5, scoring='r2')
print(f"R¬≤ CV: {cv_scores.mean():.3f} ¬± {cv_scores.std():.3f}")

# Test final
test_score = pipeline_final.score(X_test, y_test)
print(f"R¬≤ Test: {test_score:.3f}")
```

**Justificaci√≥n:**

1. **Log + Scale para features asim√©tricas**
   - Lot Area, Gr Liv Area tienen skewness >2
   - Log comprime cola, StandardScaler normaliza

2. **RobustScaler para features con outliers valiosos**
   - Garage Area tiene casas sin garaje (0) + outliers grandes
   - Total Bsmt SF similar
   - RobustScaler preserva informaci√≥n sin sesgo

3. **StandardScaler para features normales**
   - Year Built, Overall Qual razonablemente sim√©tricos
   - StandardScaler es suficiente y r√°pido

4. **GradientBoosting como modelo**
   - Robusto a escalas (podr√≠a funcionar sin scaling)
   - Pero scaling mejora convergencia y estabilidad

**Resultado final:**
- R¬≤ CV: **0.876 ¬± 0.018** (estable)
- R¬≤ Test: **0.883** (generaliza bien)
- MAE: **$18,423** (error promedio razonable)

---

## üéì Skills Desarrolladas

### T√©cnicas de Feature Engineering
- ‚úÖ **Diagn√≥stico de Escalas:** Identificaci√≥n de variables problem√°ticas mediante an√°lisis de rangos y distribuciones
- ‚úÖ **Scalers B√°sicos:** Dominio de MinMaxScaler, StandardScaler, RobustScaler y sus casos de uso
- ‚úÖ **Transformadores Avanzados:** PowerTransformer, QuantileTransformer, Log Transform para datos asim√©tricos
- ‚úÖ **An√°lisis de Skewness:** Detecci√≥n y correcci√≥n de asimetr√≠a en distribuciones
- ‚úÖ **Comparaci√≥n Emp√≠rica:** Experimentaci√≥n sistem√°tica con m√∫ltiples t√©cnicas

### Ingenier√≠a de Pipelines
- ‚úÖ **Anti-Leakage Estricto:** Protocolo split-before-fit implementado rigurosamente
- ‚úÖ **Sklearn Pipeline:** Automatizaci√≥n de preprocesamiento con fit/transform correcto
- ‚úÖ **ColumnTransformer:** Diferentes transformaciones por feature en un solo objeto
- ‚úÖ **Cross-Validation Honesta:** Validaci√≥n sin contaminar datos de test
- ‚úÖ **Serializaci√≥n:** Pipelines production-ready con `joblib`

### An√°lisis Experimental
- ‚úÖ **Dise√±o de Experimentos:** Comparaci√≥n controlada de scalers √ó algoritmos
- ‚úÖ **M√©tricas de Evaluaci√≥n:** R¬≤, MAE, an√°lisis de residuos
- ‚úÖ **Interpretaci√≥n de Resultados:** Decisiones basadas en evidencia emp√≠rica
- ‚úÖ **Visualizaci√≥n Comparativa:** Gr√°ficos before/after para comunicar impacto

### Pensamiento Cr√≠tico
- ‚úÖ **Trade-offs:** Comprensi√≥n de ventajas/desventajas de cada t√©cnica
- ‚úÖ **Contexto de Negocio:** Decisiones alineadas con tipo de datos (inmobiliarios)
- ‚úÖ **Validaci√≥n de Supuestos:** No asumir que "m√°s complejo = mejor"
- ‚úÖ **Documentaci√≥n:** Justificaci√≥n expl√≠cita de cada elecci√≥n de pipeline

---

## üí≠ Reflexi√≥n Final

### Lo Que Realmente Aprend√≠

**1. El scaling no es una f√≥rmula m√°gica**

Al inicio pensaba: "StandardScaler para todo y listo". Ahora entiendo que:
- KNN necesita scaling desesperadamente (+80% mejora)
- Random Forest no se beneficia (ahorra c√≥mputo)
- SVM requiere scaling perfecto para converger
- **La t√©cnica correcta depende del algoritmo Y los datos**

**2. Los outliers cuentan historias**

Antes ve√≠a outliers como "errores a eliminar". Ahora s√© que:
- Casa de $755k no es error, es mansi√≥n leg√≠tima
- Terreno de 215k sq ft no es ruido, es oportunidad de desarrollo
- **RobustScaler preserva informaci√≥n valiosa**
- Eliminar outliers = perder contexto del mercado de lujo

**3. El leakage es silencioso y mortal**

La diferencia de 4.6% en R¬≤ parec√≠a peque√±a, pero:
- En producci√≥n = modelo que falla en casos reales
- En stakeholders = p√©rdida de confianza ("prometiste 0.85, diste 0.81")
- En competencia = diferencia entre ganar y perder
- **Pipeline no es opcional, es obligatorio**

**4. La transformaci√≥n correcta > scaler complejo**

PowerTransformer gan√≥ NO por ser avanzado, sino porque:
- SalePrice tiene distribuci√≥n log-normal natural
- Transformar a normal mejora supuestos de regresi√≥n
- **Entender los datos > aplicar t√©cnicas fancy**

### La Lecci√≥n M√°s Importante

> **"El mejor scaler no es el m√°s complejo, es el que entiende tus datos."**

**Tres preguntas que ahora me hago SIEMPRE:**

1. **¬øQu√© algoritmo voy a usar?**
   - KNN/SVM ‚Üí NECESITO scaling
   - Random Forest ‚Üí Puedo salt√°rmelo
   - Regresi√≥n lineal ‚Üí Depende de interpretabilidad

2. **¬øC√≥mo se distribuyen mis features?**
   - Normal ‚Üí StandardScaler
   - Asim√©trica ‚Üí Log/PowerTransformer
   - Outliers valiosos ‚Üí RobustScaler

3. **¬øEstoy introduciendo leakage?**
   - ¬øFit antes del split? ‚Üí ‚ùå STOP
   - ¬øUsando Pipeline? ‚Üí ‚úÖ Safe
   - ¬øCV respeta el pipeline? ‚Üí ‚úÖ Validaci√≥n honesta

---

## üîó Enlaces y Referencias

### Datasets y Recursos
- [**Ames Housing Dataset**](https://www.kaggle.com/datasets/shashanknecrothapa/ames-housing-dataset) ‚Äî Dataset oficial de Kaggle
- [**Kaggle - Scaling and Normalization**](https://www.kaggle.com/learn/feature-engineering) ‚Äî Tutorial interactivo

### Documentaci√≥n T√©cnica
- [**Scikit-learn Preprocessing**](https://scikit-learn.org/stable/modules/preprocessing.html) ‚Äî Gu√≠a completa de transformaciones
- [**PowerTransformer**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html) ‚Äî Box-Cox y Yeo-Johnson
- [**QuantileTransformer**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.QuantileTransformer.html) ‚Äî Transformaci√≥n por cuantiles
- [**Pipeline Documentation**](https://scikit-learn.org/stable/modules/compose.html) ‚Äî Composici√≥n de transformadores

### Material del Curso
- [**Feature Scaling & Anti-Leakage Pipeline**](https://juanfkurucz.com/ucu-id/ut2/06-feature-scaling-pipeline/) ‚Äî Material oficial del curso UCU

### Papers y Lecturas
- **Box & Cox (1964):** "An Analysis of Transformations" ‚Äî Transformaciones de potencia
- **Yeo & Johnson (2000):** "A New Family of Power Transformations" ‚Äî Extensi√≥n de Box-Cox
- **G√©ron (2019):** "Hands-On Machine Learning" Cap. 2 ‚Äî Feature Scaling en pr√°ctica

---

## üîó Informaci√≥n del Proyecto

**Contexto Acad√©mico:**
- **Curso**: Calidad & √âtica de Datos - UT2  
- **Instituci√≥n**: Universidad Cat√≥lica del Uruguay  
- **Instructor**: Juan F. Kurucz  
- **Pr√°ctica**: [06 - Feature Scaling & Anti-Leakage Pipeline](https://juanfkurucz.com/ucu-id/ut2/06-feature-scaling-pipeline/)

**Alcance del Proyecto:**
- Dataset completo Ames Housing (2,930 propiedades √ó 82 variables)
- Comparaci√≥n experimental de 5+ scalers y transformadores
- Pipeline production-ready con anti-leakage estricto
- An√°lisis de impacto en 4 algoritmos diferentes (Linear, KNN, SVM, RF)

**Archivos Generados:**
- `feature_scaling_pipeline.ipynb` ‚Äî Notebook completo con experimentos
- `pipeline_final.pkl` ‚Äî Pipeline serializado para producci√≥n
- `visualizations/` ‚Äî 9 gr√°ficos comparativos (PNG)
- `docs/scaling_decisions.md` ‚Äî Justificaci√≥n de elecciones

---

<div align="center">

**‚öñÔ∏è Desarrollado con** ‚ù§Ô∏è **y** ‚òï **usando Python, Scikit-learn y mucha experimentaci√≥n**

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![Scikit-learn](https://img.shields.io/badge/Scikit--learn-1.3+-orange.svg)](https://scikit-learn.org/)
[![Pandas](https://img.shields.io/badge/Pandas-2.0+-green.svg)](https://pandas.pydata.org/)

*Proyecto acad√©mico ‚Äî Universidad Cat√≥lica del Uruguay ‚Äî 2025*

</div>
