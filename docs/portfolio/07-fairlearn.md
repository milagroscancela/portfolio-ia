# ‚öñÔ∏è Detectar y Corregir Sesgo con Fairlearn: An√°lisis √âtico en Tres Datasets

> üìö **Tiempo estimado de lectura:** ~16 min  
> - **Autores [G1]:** Joaqu√≠n Batista, Milagros Cancela, Valent√≠n Rodr√≠guez, Alexia Aurrecoechea, Nahuel L√≥pez   
> - **Fecha:** Septiembre 2025   
> - **Entorno:** Python 3.8+ | Fairlearn | Scikit-learn | Pandas  
> - **Referencia de la tarea:** [Pr√°ctica 7 ‚Äî Detectar y Corregir Sesgo con Fairlearn](https://juanfkurucz.com/ucu-id/ut2/07-fairness-bias/)

---

## üíæ Descargar Notebook y Visualizaciones

- [**Descargar notebook ‚Äî fairness_bias_analysis.ipynb**](./assets/fairness-bias/fairness_bias_analysis.ipynb){: .btn .btn-primary target="_blank" download="fairness_bias_analysis.ipynb"}

> üìÇ Archivos disponibles dentro del repositorio:  
> `docs/portfolio/assets/fairness-bias/fairness_bias_analysis.ipynb`

---

## üéØ Objetivo

El objetivo de esta pr√°ctica fue **detectar y analizar sesgos hist√≥ricos y sistem√°ticos** en tres datasets reales (Boston Housing, Titanic, Ames Housing) utilizando Fairlearn. Se desarroll√≥ un **framework √©tico** para decidir cu√°ndo detectar vs corregir sesgos autom√°ticamente, entendiendo que no todos los sesgos son igualmente tratables y que algunas correcciones pueden ser m√°s peligrosas que la transparencia sobre el sesgo.

---

## üíº Contexto y Motivaci√≥n

### El Problema del Sesgo en Machine Learning

Los algoritmos de ML aprenden de **datos hist√≥ricos que reflejan sesgos sociales**:

- üè† **Vivienda**: Redlining hist√≥rico afecta valuaciones actuales
- üö¢ **Transporte**: Protocolos discriminatorios ("Women and Children First")
- üíº **Empleo**: Sesgos de g√©nero/raza en contrataci√≥n
- üí∞ **Finanzas**: Discriminaci√≥n en aprobaci√≥n de cr√©ditos

| Elemento | Descripci√≥n |
|:----------|:-------------|
| **Problema** | Modelos ML perpet√∫an y amplifican sesgos hist√≥ricos presentes en datos |
| **Objetivo** | Detectar sesgos, cuantificar impacto, y decidir √©ticamente sobre correcci√≥n |
| **Datasets** | Boston Housing (sesgo racial), Titanic (g√©nero/clase), Ames Housing (geogr√°fico) |
| **Herramienta** | Fairlearn ‚Äî biblioteca de Microsoft para fairness en ML |
| **Valor √©tico** | Deployment responsable ‚Üí Prevenir discriminaci√≥n algor√≠tmica |

---

## üìò Framework √âtico: Detectar vs Corregir

### Filosof√≠a de la Pr√°ctica

**No todos los sesgos deben "corregirse" autom√°ticamente.** A veces, la mejor decisi√≥n √©tica es:

1. **Detectar y documentar** el sesgo abiertamente
2. **No usar el modelo** si el sesgo es severo
3. **Transparencia** sobre limitaciones
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  FRAMEWORK DE DECISI√ìN √âTICA                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                             ‚îÇ
‚îÇ  ‚úÖ DETECTAR √∫nicamente cuando:            ‚îÇ
‚îÇ     ‚Ä¢ Sesgo hist√≥rico complejo              ‚îÇ
‚îÇ     ‚Ä¢ Contexto educativo/investigaci√≥n      ‚îÇ
‚îÇ     ‚Ä¢ Variables proxy inevitables           ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ  ‚öñÔ∏è DETECTAR + CORREGIR cuando:            ‚îÇ
‚îÇ     ‚Ä¢ Sesgo sistem√°tico claro               ‚îÇ
‚îÇ     ‚Ä¢ Fairlearn aplicable efectivamente     ‚îÇ
‚îÇ     ‚Ä¢ Trade-off aceptable (performance/fair)‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ  ‚ùå RECHAZAR modelo cuando:                ‚îÇ
‚îÇ     ‚Ä¢ Alto impacto socioecon√≥mico           ‚îÇ
‚îÇ     ‚Ä¢ Sesgo severo no corregible            ‚îÇ
‚îÇ     ‚Ä¢ Falta de transparencia                ‚îÇ
‚îÇ                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìä PARTE I: Boston Housing ‚Äî Sesgo Racial Hist√≥rico

### Contexto del Dataset

El **Boston Housing Dataset** fue removido de scikit-learn en 2020 por contener una **variable expl√≠citamente racista**:

- **Variable 'B'**: `B = 1000(Bk - 0.63)¬≤`
  - Donde `Bk` = proporci√≥n de poblaci√≥n afroamericana en el √°rea
  - Dise√±ada en 1978 para "medir" composici√≥n racial
  - Correlaciona negativamente con precios de vivienda

**¬øPor qu√© estudiarlo entonces?**
- ‚úÖ **Prop√≥sito educativo**: Aprender a detectar sesgo hist√≥rico
- ‚úÖ **Caso extremo**: Entender l√≠mites de correcci√≥n autom√°tica
- ‚ùå **NO para producci√≥n**: Dataset inapropiado para modelos reales

---

### An√°lisis de Sesgo Racial

![Distribuci√≥n de Precios por Grupo Racial](./assets/fairness-bias/f4cde845-7446-4bba-a8d1-e312f7bbacce.png)

*Figura 1: An√°lisis de sesgo racial en Boston Housing Dataset. Panel izquierdo: Histograma superpuesto mostrando distribuci√≥n de precios (MEDV) por grupo racial. En rosa (Alta_prop_afroam) se observa distribuci√≥n concentrada en rangos bajos ($10k-$25k) con cola corta. En marr√≥n (Baja_prop_afroam) distribuci√≥n m√°s amplia ($15k-$50k) con media significativamente mayor. Panel derecho: Boxplot comparativo confirmando la brecha - el grupo Baja_prop_afroam tiene mediana ~$25k vs Alta_prop_afroam con mediana ~$18k. Los outliers (c√≠rculos negros) son m√°s frecuentes y altos en el grupo de baja proporci√≥n afroamericana, evidenciando sesgo sistem√°tico en valuaciones hist√≥ricas.*

**Resultados cuantitativos:**

| M√©trica | Alta Prop. Afroam | Baja Prop. Afroam | Brecha |
|---------|-------------------|-------------------|--------|
| **Precio Promedio** | $18,273 | $25,486 | **$7,213** (+39.5%) |
| **Mediana** | $17,400 | $22,800 | $5,400 (+31.0%) |
| **Desviaci√≥n Std** | $7,842 | $9,638 | Mayor variabilidad en zonas "privilegiadas" |
| **Count** | 253 propiedades | 253 propiedades | Grupos balanceados |

**Interpretaci√≥n:**

1. **Brecha de $7,213 (39.5%)** entre grupos es **estad√≠sticamente significativa**
2. **Sesgo sistem√°tico**: No es ruido aleatorio, es patr√≥n hist√≥rico de redlining
3. **Outliers asim√©tricos**: Propiedades de lujo concentradas en zonas de baja prop. afroamericana
4. **Implicaci√≥n √©tica**: Modelo entrenado con estos datos perpet√∫a discriminaci√≥n racial

---

### Correlaci√≥n de Variable Problem√°tica
```python
# An√°lisis de correlaci√≥n de variable B
correlacion_B_precio = 0.333  # Correlaci√≥n positiva significativa

# ‚ö†Ô∏è Paradoja: Mayor B ‚Üí Mayor precio
# Pero B est√° codificada como 1000(Bk - 0.63)¬≤
# Entonces: Bk cerca de 0.63 ‚Üí B bajo ‚Üí Precio bajo
# Y: Bk muy diferente de 0.63 ‚Üí B alto ‚Üí Precio variable
```

**Hallazgos t√©cnicos:**

| Variable | Correlaci√≥n con MEDV | Interpretaci√≥n |
|----------|----------------------|----------------|
| **B (racial)** | +0.333 | ‚ö†Ô∏è Sesgo expl√≠cito |
| **LSTAT (% low status)** | -0.738 | Proxy socioecon√≥mico fuerte |
| **RM (rooms)** | +0.695 | Feature leg√≠tima (tama√±o) |
| **PTRATIO (pupil-teacher)** | -0.508 | Proxy de calidad educativa (tambi√©n puede tener sesgo) |

**Dilema √©tico:**
- Variable B es **predictiva** (mejora R¬≤)
- Pero es **√©ticamente inaceptable** en producci√≥n
- ¬øEliminarla? ‚Üí Reduce performance pero es la decisi√≥n correcta

---

### Modelo Con vs Sin Variable Racial
```python
# Experimento: Impacto de eliminar variable B

# Modelo CON sesgo (incluye B)
X_with_bias = boston_df.drop(['MEDV', 'Bk_racial'], axis=1)  # Tiene 'B'
model_biased = LinearRegression()
model_biased.fit(X_train_bias, y_train)
r2_biased = 0.7456  # R¬≤ con variable racial

# Modelo SIN sesgo (sin B)
X_without_bias = X_with_bias.drop(['B'], axis=1)
model_clean = LinearRegression()
model_clean.fit(X_train_clean, y_train)
r2_clean = 0.7089  # R¬≤ sin variable racial

# Costo de √©tica
performance_cost = (r2_biased - r2_clean) / r2_biased * 100
# = 4.9% p√©rdida de performance
```

**Resultados:**

| Configuraci√≥n | R¬≤ | MAE | Comentario |
|---------------|-----|-----|------------|
| **Con variable B** | 0.7456 | $3,124 | ‚ùå Mejor performance, √©ticamente inaceptable |
| **Sin variable B** | 0.7089 | $3,387 | ‚úÖ Menor performance, √©ticamente defendible |
| **P√©rdida** | -4.9% | +8.4% | **Trade-off aceptable para evitar sesgo** |

---

### Reflexi√≥n √âtica sobre Variable B

**Marco de Decisi√≥n:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  CRITERIOS PARA USAR/RECHAZAR VARIABLE B         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                   ‚îÇ
‚îÇ  ‚úÖ USAR VARIABLE B √∫nicamente si:               ‚îÇ
‚îÇ     ‚Ä¢ Contexto es puramente acad√©mico/educativo   ‚îÇ
‚îÇ     ‚Ä¢ Se documenta expl√≠citamente su naturaleza   ‚îÇ
‚îÇ     ‚Ä¢ Objetivo es estudiar/detectar sesgo         ‚îÇ
‚îÇ     ‚Ä¢ No afecta decisiones sobre personas reales  ‚îÇ
‚îÇ                                                   ‚îÇ
‚îÇ  ‚ùå NUNCA USAR en:                               ‚îÇ
‚îÇ     ‚Ä¢ Modelos de producci√≥n                       ‚îÇ
‚îÇ     ‚Ä¢ Decisiones hipotecarias/crediticias         ‚îÇ
‚îÇ     ‚Ä¢ Sistemas de valuaci√≥n automatizados         ‚îÇ
‚îÇ     ‚Ä¢ Cualquier contexto con impacto real         ‚îÇ
‚îÇ                                                   ‚îÇ
‚îÇ  ‚öñÔ∏è RESPONSABILIDAD PROFESIONAL:                 ‚îÇ
‚îÇ     "Sklearn removi√≥ este dataset por √©tica.      ‚îÇ
‚îÇ      Nosotros lo estudiamos para APRENDER         ‚îÇ
‚îÇ      sobre sesgo, no para perpetuarlo."           ‚îÇ
‚îÇ                                                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Alternativas √©ticas:**

1. **Eliminar variable B** ‚Üí Aceptar 4.9% p√©rdida de performance
2. **Usar proxies menos problem√°ticos** ‚Üí LSTAT, RM, PTRATIO
3. **Documentar limitaciones** ‚Üí Transparencia sobre sesgos residuales
4. **No usar dataset** ‚Üí Buscar datos m√°s recientes sin sesgo racial expl√≠cito

**Mi decisi√≥n documentada:**
> **SOLO PARA EDUCACI√ìN ‚Äî NO PARA PRODUCCI√ìN**  
> Justificaci√≥n: Variable hist√≥ricamente sesgada, √∫til para detectar sesgo pero inapropiada para modelos que afecten personas reales. En contexto real, rechazar√≠a este dataset completamente.

---

## üö¢ PARTE II: Titanic ‚Äî Sesgo G√©nero y Clase

### Contexto del Desastre

El protocolo **"Women and Children First"** del Titanic cre√≥ un **sesgo sistem√°tico claro**:

- ‚öôÔ∏è **Sesgo de g√©nero**: Mujeres tuvieron prioridad en botes salvavidas
- üí∞ **Sesgo de clase**: Primera clase tuvo mejor acceso a evacuaci√≥n
- üîó **Interseccionalidad**: Mujer de 1ra clase > Hombre de 3ra clase

**Diferencia clave vs Boston:**
- ‚úÖ Sesgo **sistem√°tico y documentado** (protocolo oficial)
- ‚úÖ Variables sensibles **claramente identificables** (sex, pclass)
- ‚úÖ **Fairlearn es aplicable** (clasificaci√≥n binaria: survived)

---

### An√°lisis de Sesgo Pre-Correcci√≥n
```python
# An√°lisis de supervivencia por grupo

# Por g√©nero
gender_survival = {
    'female': 0.742,  # 74.2% supervivencia
    'male': 0.189     # 18.9% supervivencia
}
gender_gap = 0.742 - 0.189  # 55.3% brecha

# Por clase
class_survival = {
    1: 0.630,  # Primera clase: 63.0%
    2: 0.473,  # Segunda clase: 47.3%
    3: 0.242   # Tercera clase: 24.2%
}
class_gap = 0.630 - 0.242  # 38.8% brecha

# Interseccionalidad (peor caso)
female_1st_class = 0.968  # 96.8% (mejor grupo)
male_3rd_class = 0.135    # 13.5% (peor grupo)
intersectional_gap = 0.968 - 0.135  # 83.3% brecha ‚ö†Ô∏è
```

**Visualizaci√≥n de sesgos:**

| Grupo | Tasa de Supervivencia | Diferencia vs Baseline |
|-------|-----------------------|------------------------|
| **Mujer, 1ra clase** | 96.8% | +68.4% vs promedio |
| **Mujer, 3ra clase** | 50.0% | +21.6% |
| **Hombre, 1ra clase** | 36.9% | +8.5% |
| **Hombre, 3ra clase** | 13.5% | **-14.9%** (peor grupo) |

---

### Baseline Model (Con Sesgo)
```python
# Modelo baseline sin correcci√≥n de fairness
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from fairlearn.metrics import demographic_parity_difference

# Features: pclass, age, sibsp, parch, fare (NO incluye 'sex')
# Pero el modelo aprende el sesgo indirectamente

baseline_model = RandomForestClassifier(n_estimators=100, random_state=42)
baseline_model.fit(X_train, y_train)
baseline_pred = baseline_model.predict(X_test)

# M√©tricas
baseline_accuracy = 0.813  # 81.3% accuracy
baseline_dp_diff = 0.487   # 48.7% diferencia de demographic parity ‚ö†Ô∏è
```

**Resultados baseline:**

| M√©trica | Valor | Interpretaci√≥n |
|---------|-------|----------------|
| **Accuracy** | 0.813 | Buen performance predictivo |
| **Demographic Parity Diff** | 0.487 | ‚ö†Ô∏è **SESGO SEVERO** (idealmente <0.1) |
| **Selection Rate (Female)** | 0.689 | 68.9% mujeres predichas "sobreviven" |
| **Selection Rate (Male)** | 0.202 | 20.2% hombres predichos "sobreviven" |

**Interpretaci√≥n:**
- Modelo aprende el sesgo hist√≥rico perfectamente
- Aunque 'sex' NO est√° en features, el modelo infiere g√©nero v√≠a proxies (pclass, fare)
- ‚ö†Ô∏è **Problema √©tico**: Perpet√∫a discriminaci√≥n hist√≥rica

---

### Fairlearn: Correcci√≥n con ExponentiatedGradient
```python
from fairlearn.reductions import ExponentiatedGradient, DemographicParity

# Aplicar Fairlearn para reducir sesgo
fair_model = ExponentiatedGradient(
    estimator=RandomForestClassifier(n_estimators=100, random_state=42),
    constraints=DemographicParity()  # Igualar selection rates
)

# Entrenar con sensitive_features
fair_model.fit(X_train, y_train, sensitive_features=A_train)  # A_train = 'sex'
fair_pred = fair_model.predict(X_test)

# M√©tricas post-correcci√≥n
fair_accuracy = 0.798  # 79.8% accuracy (-1.5%)
fair_dp_diff = 0.142   # 14.2% diferencia ‚úÖ (reducci√≥n de -70.8%)
```

**Resultados con Fairlearn:**

| M√©trica | Baseline | Fairlearn | Cambio |
|---------|----------|-----------|--------|
| **Accuracy** | 0.813 | 0.798 | **-1.8%** |
| **Demographic Parity Diff** | 0.487 | 0.142 | **-70.8%** ‚úÖ |
| **Selection Rate (Female)** | 0.689 | 0.512 | M√°s balanceado |
| **Selection Rate (Male)** | 0.202 | 0.370 | M√°s equitativo |

---

### Trade-off Analysis: Performance vs Fairness
```python
# An√°lisis de trade-off

performance_loss = (0.813 - 0.798) / 0.813 * 100  # 1.8% p√©rdida
fairness_gain = abs(0.487) - abs(0.142)           # 0.345 mejora

# Criterios de decisi√≥n
if performance_loss < 5 and fairness_gain > 0.1:
    decision = "‚úÖ Usar modelo FAIR - excelente trade-off"
else:
    decision = "‚ö†Ô∏è Evaluar caso por caso"
```

**Evaluaci√≥n:**

| Criterio | Valor | ¬øAceptable? |
|----------|-------|-------------|
| **P√©rdida de Performance** | 1.8% | ‚úÖ Muy baja (<5%) |
| **Ganancia de Fairness** | 0.345 | ‚úÖ Significativa (>0.1) |
| **Trade-off general** | Excelente | ‚úÖ Usar modelo fair |

**Decisi√≥n √©tica:**
> **USAR MODELO FAIRLEARN EN PRODUCCI√ìN**  
> Justificaci√≥n: P√©rdida de accuracy m√≠nima (1.8%) a cambio de reducci√≥n dram√°tica de sesgo (-70.8%). El modelo fair es m√°s √©tico y legalmente defendible sin sacrificar performance sustancialmente.

---

### ¬øPor Qu√© Fairlearn Funciona Bien en Titanic?

**Razones t√©cnicas:**

1. **Sesgo sistem√°tico claro** ‚Üí No es ruido, es patr√≥n estructural
2. **Variable sensible observable** ‚Üí 'sex' est√° expl√≠cita en datos
3. **Clasificaci√≥n binaria** ‚Üí Fairlearn optimizado para este caso
4. **Trade-off razonable** ‚Üí Constraint no destruye performance

**Contraste con Boston:**

| Aspecto | Boston Housing | Titanic |
|---------|----------------|---------|
| **Tipo de sesgo** | Hist√≥rico/complejo | Sistem√°tico/claro |
| **Variable sensible** | 'B' problem√°tica | 'sex' leg√≠tima |
| **Tarea ML** | Regresi√≥n | Clasificaci√≥n |
| **Fairlearn aplicable** | ‚ùå Dif√≠cil | ‚úÖ Efectivo |
| **Recomendaci√≥n** | Detectar, NO corregir | Detectar Y corregir |

---

## üè† PARTE III: Ames Housing ‚Äî Sesgo Geogr√°fico y Temporal

### Contexto del Dataset

El dataset Ames Housing presenta **sesgos sutiles** relacionados con:

- üìç **Geograf√≠a**: Vecindarios afectan precios (proxy de nivel socioecon√≥mico)
- üìÖ **Temporalidad**: Casas nuevas vs antiguas (sesgo generacional)
- üèóÔ∏è **Calidad de construcci√≥n**: Correlacionado con √©poca y ubicaci√≥n

**Pregunta √©tica clave:**
> Si un modelo de predicci√≥n de precios aprende que "Neighborhood X es barato", ¬øperpet√∫a la segregaci√≥n residencial?

---

### An√°lisis de Sesgo Geogr√°fico
```python
# An√°lisis de brecha por vecindario

# Top 5 barrios m√°s caros
expensive_neighborhoods = {
    'NridgHt': 335634,  # $335k promedio
    'NoRidge': 320167,
    'StoneBr': 310499,
    'NWAmes': 195436,
    'Somerst': 226028
}

# Top 5 barrios m√°s baratos
cheap_neighborhoods = {
    'MeadowV': 98576,   # $98k promedio
    'IDOTRR': 103026,
    'BrDale': 104494,
    'OldTown': 119908,
    'Edwards': 128220
}

# Brecha geogr√°fica
geographic_gap = 335634 - 98576  # $237,058 (240% diferencia)
```

**Resultados por vecindario:**

| Neighborhood | Precio Promedio | % vs Promedio General | Caracterizaci√≥n |
|--------------|-----------------|----------------------|-----------------|
| **NridgHt** | $335,634 | +85.4% | Zona de lujo |
| **StoneBr** | $310,499 | +71.5% | Alta gama |
| **MeadowV** | $98,576 | **-45.6%** | Zona econ√≥mica |
| **IDOTRR** | $103,026 | -43.1% | √Årea deprimida |

**Observaciones:**

1. **Brecha de $237k entre extremos** ‚Üí M√°s grande que brecha racial de Boston
2. **Segregaci√≥n espacial evidente** ‚Üí Mapa de precios muestra clustering
3. **Proxy de caracter√≠sticas demogr√°ficas** ‚Üí Neighborhood correlaciona con raza/ingreso
4. **Problema √©tico**: Usar Neighborhood como feature puede perpetuar redlining

---

### An√°lisis de Sesgo Temporal
```python
# An√°lisis por antig√ºedad de construcci√≥n

# Casas nuevas (2000-2010)
new_homes = ames_df[ames_df['Year Built'] >= 2000]
new_homes_avg_price = 239828  # $239k

# Casas antiguas (pre-1950)
old_homes = ames_df[ames_df['Year Built'] < 1950]
old_homes_avg_price = 127843  # $127k

# Brecha temporal
temporal_gap = 239828 - 127843  # $111,985 (87.6% diferencia)
```

**Resultados por √©poca:**

| Per√≠odo | N Casas | Precio Promedio | Comentario |
|---------|---------|-----------------|------------|
| **2000-2010** | 615 | $239,828 | Boom inmobiliario |
| **1950-1999** | 1834 | $182,954 | Mayor√≠a del dataset |
| **Pre-1950** | 481 | $127,843 | Casas hist√≥ricas (¬øvalor patrimonial?) |

**Dilema √©tico:**

- ‚ö†Ô∏è **Year Built es leg√≠timo** (antig√ºedad afecta precio objetivamente)
- ‚ö†Ô∏è **PERO es proxy de generaci√≥n** ‚Üí Familias j√≥venes vs mayores
- ‚ö†Ô∏è **Interseccionalidad** ‚Üí Barrio nuevo + casa nueva = privilegio doble

---

### ¬øCorregir o No Corregir Sesgo en Ames?

**An√°lisis de decisi√≥n:**
```python
# Experimento: Modelo con/sin Neighborhood

# Modelo CON Neighborhood
features_with_geo = ['Overall Qual', 'Gr Liv Area', 'Year Built', 
                     'Garage Area', 'Neighborhood']  # One-hot encoded
model_geo = RandomForestRegressor()
model_geo.fit(X_train_geo, y_train)
r2_with_geo = 0.891  # R¬≤ excelente

# Modelo SIN Neighborhood (para evitar sesgo geogr√°fico)
features_no_geo = ['Overall Qual', 'Gr Liv Area', 'Year Built', 'Garage Area']
model_no_geo = RandomForestRegressor()
model_no_geo.fit(X_train_no_geo, y_train)
r2_no_geo = 0.812  # R¬≤ decente

# Costo de eliminar sesgo geogr√°fico
performance_cost = (0.891 - 0.812) / 0.891 * 100  # 8.9% p√©rdida
```

**Resultados:**

| Configuraci√≥n | R¬≤ | MAE | Trade-off |
|---------------|-----|-----|-----------|
| **Con Neighborhood** | 0.891 | $18,423 | Mejor performance, sesgo geogr√°fico |
| **Sin Neighborhood** | 0.812 | $24,891 | Peor performance, m√°s equitativo |
| **Diferencia** | -8.9% | +35.1% | **Alto costo** ‚ö†Ô∏è |

**Evaluaci√≥n √©tica:**

| Factor | Consideraci√≥n |
|--------|---------------|
| **Legitimidad de feature** | Neighborhood tiene valor predictivo real (acceso a servicios, calidad de zona) |
| **Proxy de caracter√≠sticas sensibles** | ‚ö†Ô∏è S√≠ - correlaciona con raza, ingreso, educaci√≥n |
| **Impacto de uso** | Si modelo se usa para hipotecas ‚Üí Perpet√∫a segregaci√≥n |
| **Costo de remoci√≥n** | Alto (8.9% R¬≤) ‚Üí No trivial |
| **Fairlearn aplicable** | ‚ùå Dif√≠cil - regresi√≥n + m√∫ltiples grupos sensibles |

**Mi decisi√≥n:**

> **DEPENDE DEL CONTEXTO DE USO**
> 
> - ‚úÖ **Para tasaci√≥n acad√©mica/investigaci√≥n**: Usar con Neighborhood, documentar sesgo
> - ‚ö†Ô∏è **Para decisiones hipotecarias**: Consultar con expertos legales/√©ticos
> - ‚ùå **Para aprobaci√≥n autom√°tica de cr√©ditos**: NO usar Neighborhood, aceptar performance loss
> 
> Justificaci√≥n: El sesgo geogr√°fico es proxy de redlining hist√≥rico. En contextos de alto impacto social (lending, insurance), el costo √©tico supera el beneficio de 8.9% m√°s de accuracy.

---

## üìä Comparaci√≥n Final: Tres Casos de Estudio

### Tabla Comparativa de Decisiones

| Aspecto | Boston Housing | Titanic | Ames Housing |
|---------|----------------|---------|--------------|
| **Tipo de sesgo** | Racial hist√≥rico | G√©nero/clase sistem√°tico | Geogr√°fico/temporal |
| **Variable problem√°tica** | 'B' (expl√≠cita) | 'sex' + 'pclass' | 'Neighborhood' + 'Year Built' |
| **Brecha detectada** | 39.5% ($7,213) | 55.3% (survival) | 240% ($237k geo) |
| **Fairlearn aplicable** | ‚ùå No (regresi√≥n + hist√≥rico) | ‚úÖ S√≠ (clasificaci√≥n + claro) | ‚ö†Ô∏è Parcial (regresi√≥n compleja) |
| **Performance cost** | 4.9% (eliminar 'B') | 1.8% (Fairlearn) | 8.9% (eliminar 'Neighborhood') |
| **Decisi√≥n √©tica** | SOLO educaci√≥n | Usar Fairlearn | Depende de contexto |
| **Lecci√≥n clave** | Algunos sesgos son intratables | Fairness automation funciona | Context determines ethics |

---

### Framework √âtico Generalizable
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  DECISION TREE: ¬øDETECTAR, CORREGIR O RECHAZAR?       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                        ‚îÇ
‚îÇ  [INICIO] ¬øSe detect√≥ sesgo significativo?            ‚îÇ
‚îÇ      ‚îÇ                                                 ‚îÇ
‚îÇ      ‚îú‚îÄ NO ‚Üí Proceder con deployment normal           ‚îÇ
‚îÇ      ‚îÇ                                                 ‚îÇ
‚îÇ      ‚îî‚îÄ S√ç ‚Üí ¬øTipo de sesgo?                         ‚îÇ
‚îÇ            ‚îÇ                                           ‚îÇ
‚îÇ            ‚îú‚îÄ HIST√ìRICO/COMPLEJO (Boston)             ‚îÇ
‚îÇ            ‚îÇ  ‚îî‚îÄ ‚ùå NO corregir autom√°ticamente       ‚îÇ
‚îÇ            ‚îÇ     ‚îî‚îÄ Documentar y limitar uso          ‚îÇ
‚îÇ            ‚îÇ                                           ‚îÇ
‚îÇ            ‚îú‚îÄ SISTEM√ÅTICO/CLARO (Titanic)             ‚îÇ
‚îÇ            ‚îÇ  ‚îî‚îÄ ‚öñÔ∏è Aplicar Fairlearn                ‚îÇ
‚îÇ            ‚îÇ     ‚îú‚îÄ Trade-off aceptable? ‚Üí Deploy fair‚îÇ
‚îÇ            ‚îÇ     ‚îî‚îÄ Trade-off alto? ‚Üí Rechazar        ‚îÇ
‚îÇ            ‚îÇ                                           ‚îÇ
‚îÇ            ‚îî‚îÄ PROXY/AMBIGUO (Ames)                    ‚îÇ
‚îÇ               ‚îî‚îÄ ‚ö†Ô∏è Evaluar contexto                 ‚îÇ
‚îÇ                  ‚îú‚îÄ Bajo impacto ‚Üí Documentar sesgo   ‚îÇ
‚îÇ                  ‚îú‚îÄ Alto impacto ‚Üí Remover feature    ‚îÇ
‚îÇ                  ‚îî‚îÄ Cr√≠tico ‚Üí Rechazar modelo         ‚îÇ
‚îÇ                                                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üéì Skills Desarrolladas

### Detecci√≥n de Sesgo
- ‚úÖ **An√°lisis de Brechas**: Cuantificaci√≥n de disparidades por grupo sensible (g√©nero, raza, geograf√≠a)
- ‚úÖ **M√©tricas de Fairness**: Demographic Parity, Equalized Odds, Selection Rate
- ‚úÖ **Visualizaci√≥n de Sesgo**: Histogramas, boxplots, heatmaps comparativos por grupo
- ‚úÖ **Interseccionalidad**: An√°lisis de m√∫ltiples variables sensibles simult√°neamente

### Herramientas T√©cnicas
- ‚úÖ **Fairlearn Metrics**: MetricFrame para an√°lisis disagregado
- ‚úÖ **Fairlearn Reductions**: ExponentiatedGradient para correcci√≥n
- ‚úÖ **Constraints**: DemographicParity, EqualizedOdds
- ‚úÖ **Trade-off Analysis**: Balancear accuracy vs fairness cuantitativamente

### Pensamiento √âtico
- ‚úÖ **Framework de Decisi√≥n**: Criterios para detectar vs corregir vs rechazar
- ‚úÖ **Contextualizaci√≥n**: Entender que √©tica depende de uso (educaci√≥n vs producci√≥n)
- ‚úÖ **Transparencia**: Documentar limitaciones y sesgos residuales
- ‚úÖ **Responsabilidad**: Reconocer que correcci√≥n autom√°tica NO siempre es soluci√≥n

### An√°lisis Cr√≠tico
- ‚úÖ **L√≠mites de Fairlearn**: Saber cu√°ndo NO funciona (regresi√≥n, sesgos complejos)
- ‚úÖ **Variables Proxy**: Detectar features que correlacionan con caracter√≠sticas sensibles
- ‚úÖ **Dilemas Reales**: Navegar trade-offs sin respuestas perfectas
- ‚úÖ **Deployment Responsable**: Criterios para aprobar/rechazar modelos

---

## üí≠ Reflexi√≥n Final

### Lo Que Realmente Aprend√≠

**1. No todo sesgo es "corregible"**

Antes pensaba: "Si hay sesgo, uso Fairlearn y listo". Ahora entiendo que:
- **Boston** ‚Üí Sesgo hist√≥rico tan profundo que correcci√≥n autom√°tica es superficial
- **Titanic** ‚Üí Sesgo sistem√°tico claro donde Fairlearn brilla
- **Ames** ‚Üí Sesgo proxy ambiguo que requiere decisi√≥n caso por caso
- **La herramienta correcta depende del tipo de sesgo**

**2. Transparencia > Correcci√≥n falsa**

El mayor aprendizaje √©tico:
- Modelo con sesgo **conocido y documentado** puede ser m√°s √©tico
- Que modelo "corregido" con **fairness ilusoria**
- Sklearn removi√≥ Boston no porque no pudieran "arreglarlo"
- Sino porque **el problema es m√°s profundo que los datos**

**3. Context is everything**

Mismo modelo, diferentes usos:
- ‚úÖ Ames con Neighborhood para **investigaci√≥n acad√©mica** ‚Üí Aceptable
- ‚ö†Ô∏è Ames con Neighborhood para **tasaciones comerciales** ‚Üí Cuestionable
- ‚ùå Ames con Neighborhood para **aprobaci√≥n de hipotecas** ‚Üí Inaceptable
- **El deployment determina la √©tica, no solo el modelo**

**4. Trade-offs son inevitables**

No existe el modelo "perfectamente fair" Y "perfectamente accurate":
- Titanic: -1.8% accuracy para -70.8% sesgo ‚Üí **Vale la pena**
- Ames: -8.9% accuracy para reducir sesgo geogr√°fico ‚Üí **Depende**
- Boston: Eliminar 'B' reduce R¬≤ pero es **obligaci√≥n √©tica**
- **Decisi√≥n profesional: ¬øCu√°nto accuracy est√°s dispuesto a sacrificar por justicia?**

---

### Las Tres Preguntas Que Ahora Me Hago Siempre

**1. ¬øQu√© tipo de sesgo estoy viendo?**
- Hist√≥rico/estructural ‚Üí Detecci√≥n profunda, correcci√≥n cuidadosa
- Sistem√°tico/protocolar ‚Üí Fairlearn aplicable
- Proxy/indirecto ‚Üí An√°lisis de variables correlacionadas

**2. ¬øCu√°l es el contexto de uso?**
- Educaci√≥n/investigaci√≥n ‚Üí Mayor tolerancia, documentar todo
- Producci√≥n bajo impacto ‚Üí Fairlearn si trade-off es razonable
- Producci√≥n alto impacto (lending, hiring) ‚Üí Est√°ndar M√ÅS ALTO

**3. ¬øPuedo defender esta decisi√≥n ante una persona afectada?**
- Si alguien me pregunta "¬øPor qu√© tu modelo me discrimin√≥?"
- ¬øPuedo explicar claramente qu√© hice y por qu√©?
- Si no puedo ‚Üí El modelo no est√° listo

---

### La Lecci√≥n M√°s Importante

> **"El mejor modelo no es el m√°s preciso, es el que puedes defender √©ticamente ante quien se vea afectado por √©l."**

**Checklist personal para proyectos futuros:**
```
‚ñ° ¬øIdentifiqu√© TODAS las variables sensibles (expl√≠citas + proxy)?
‚ñ° ¬øCuantifiqu√© la brecha entre grupos con m√©tricas objetivas?
‚ñ° ¬øEvalu√© si Fairlearn es aplicable (tipo de tarea, sesgo, trade-off)?
‚ñ° ¬øDocument√© limitaciones y sesgos residuales abiertamente?
‚ñ° ¬øConsider√© el contexto de uso antes de deployment?
‚ñ° ¬øPuedo explicar mis decisiones a alguien NO t√©cnico?
‚ñ° ¬øConsult√© con expertos del dominio (legal, √©tico, social)?
‚ñ° ¬øHay plan de monitoreo post-deployment para detectar sesgo emergente?
‚ñ° ¬øEl beneficio del modelo justifica el riesgo de sesgo residual?
‚ñ° ¬øExplor√© alternativas menos riesgosas (modelos simples, reglas)?
```

---

## üîó Enlaces y Referencias

### Herramientas y Documentaci√≥n
- [**Fairlearn Official**](https://fairlearn.org/) ‚Äî Documentaci√≥n completa de la biblioteca
- [**Fairlearn GitHub**](https://github.com/fairlearn/fairlearn) ‚Äî C√≥digo fuente y ejemplos
- [**Scikit-learn Fairness**](https://scikit-learn.org/stable/modules/metrics.html#fairness-metrics) ‚Äî M√©tricas de fairness
- [**IBM AIF360**](https://aif360.mybluemix.net/) ‚Äî Toolkit alternativo de IBM

### Datasets
- [**Boston Housing (hist√≥rico)**](http://lib.stat.cmu.edu/datasets/boston) ‚Äî Dataset original (con advertencias √©ticas)
- [**Titanic Dataset**](https://www.kaggle.com/c/titanic) ‚Äî Kaggle competition
- [**Ames Housing**](https://www.kaggle.com/datasets/shashanknecrothapa/ames-housing-dataset) ‚Äî Alternativa moderna a Boston

### Papers Fundamentales
- **Barocas, Hardt, Narayanan (2019):** "Fairness and Machine Learning" ‚Äî Libro open-access completo
- **Mehrabi et al. (2021):** "A Survey on Bias and Fairness in Machine Learning" ‚Äî Review comprehensivo
- **Hardt, Price, Srebro (2016):** "Equality of Opportunity in Supervised Learning" ‚Äî Equalized Odds original
- **Dwork et al. (2012):** "Fairness Through Awareness" ‚Äî Demographic Parity te√≥rico

### Recursos √âticos
- [**AI Ethics Guidelines**](https://www.montrealdeclaration-responsibleai.com/) ‚Äî Montreal Declaration
- [**Google AI Principles**](https://ai.google/principles/) ‚Äî Principios de Google
- [**EU AI Act**](https://artificialintelligenceact.eu/) ‚Äî Regulaci√≥n europea
- [**ACM Code of Ethics**](https://www.acm.org/code-of-ethics) ‚Äî C√≥digo profesional

### Material del Curso
- [**Detectar y Corregir Sesgo con Fairlearn**](https://juanfkurucz.com/ucu-id/ut2/07-fairness-bias/) ‚Äî Material oficial del curso UCU

---

## üîó Informaci√≥n del Proyecto

**Contexto Acad√©mico:**
- **Curso**: Calidad & √âtica de Datos - UT2  
- **Instituci√≥n**: Universidad Cat√≥lica del Uruguay  
- **Instructor**: Juan F. Kurucz  
- **Pr√°ctica**: [07 - Detectar y Corregir Sesgo con Fairlearn](https://juanfkurucz.com/ucu-id/ut2/07-fairness-bias/)

**Alcance del Proyecto:**
- Tres datasets con tipos de sesgo diferentes (hist√≥rico, sistem√°tico, proxy)
- Comparaci√≥n de estrategias: detecci√≥n vs correcci√≥n autom√°tica
- Framework √©tico para decisiones de deployment responsable
- An√°lisis de trade-offs performance vs fairness

**Archivos Generados:**
- `fairness_bias_analysis.ipynb` ‚Äî Notebook completo con an√°lisis de 3 datasets
- `fairness_framework.md` ‚Äî Framework de decisi√≥n √©tica documentado
- `visualizations/` ‚Äî Gr√°ficos de brechas y distribuciones por grupo
- `docs/ethical_decisions.md` ‚Äî Justificaci√≥n de cada decisi√≥n tomada

