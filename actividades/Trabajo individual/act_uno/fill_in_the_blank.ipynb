{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33efc657",
   "metadata": {},
   "source": [
    "# ðŸ”— Tarea 4: EDA Multi-fuentes y Joins - Fill in the Blanks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375825d4",
   "metadata": {},
   "source": [
    "### ðŸ”§ Paso 1: Setup Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84592d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Setup completo para anÃ¡lisis multi-fuentes!\n"
     ]
    }
   ],
   "source": [
    "# Importar librerÃ­as que vamos a usar\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "\n",
    "# Configurar visualizaciones\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"âœ… Setup completo para anÃ¡lisis multi-fuentes!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f21067",
   "metadata": {},
   "source": [
    "### ðŸš• Paso 2: Carga de Datos desde MÃºltiples Fuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b73f144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos oficiales de NYC Taxi (dataset completo)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m trips_url = \u001b[33m\"\u001b[39m\u001b[33mhttps://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Cargar dataset oficial (~3M registros de enero 2023)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m trips = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrips_url\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# funciÃ³n para leer archivos .parquet (mÃ¡s eficiente que CSV)\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Viajes cargados: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrips.shape[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m filas, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrips.shape[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m columnas\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Columnas: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(trips.columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/web_ing_data/ia-portfolio/venv/lib/python3.12/site-packages/pandas/io/parquet.py:653\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    500\u001b[39m \u001b[38;5;129m@doc\u001b[39m(storage_options=_shared_docs[\u001b[33m\"\u001b[39m\u001b[33mstorage_options\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_parquet\u001b[39m(\n\u001b[32m    502\u001b[39m     path: FilePath | ReadBuffer[\u001b[38;5;28mbytes\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    510\u001b[39m     **kwargs,\n\u001b[32m    511\u001b[39m ) -> DataFrame:\n\u001b[32m    512\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    513\u001b[39m \u001b[33;03m    Load a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[32m    514\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    650\u001b[39m \u001b[33;03m    1    4    9\u001b[39;00m\n\u001b[32m    651\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m653\u001b[39m     impl = \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m use_nullable_dtypes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib.no_default:\n\u001b[32m    656\u001b[39m         msg = (\n\u001b[32m    657\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe argument \u001b[39m\u001b[33m'\u001b[39m\u001b[33muse_nullable_dtypes\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is deprecated and will be removed \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    658\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33min a future version.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    659\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/web_ing_data/ia-portfolio/venv/lib/python3.12/site-packages/pandas/io/parquet.py:64\u001b[39m, in \u001b[36mget_engine\u001b[39m\u001b[34m(engine)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m engine_class \u001b[38;5;129;01min\u001b[39;00m engine_classes:\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m     66\u001b[39m         error_msgs += \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m - \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(err)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/web_ing_data/ia-portfolio/venv/lib/python3.12/site-packages/pandas/io/parquet.py:167\u001b[39m, in \u001b[36mPyArrowImpl.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    164\u001b[39m     import_optional_dependency(\n\u001b[32m    165\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m, extra=\u001b[33m\"\u001b[39m\u001b[33mpyarrow is required for parquet support.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    166\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# import utils to register the pyarrow extension types\u001b[39;00m\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrays\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mextension_types\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/web_ing_data/ia-portfolio/venv/lib/python3.12/site-packages/pyarrow/parquet/__init__.py:20\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Licensed to the Apache Software Foundation (ASF) under one\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# or more contributor license agreements.  See the NOTICE file\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# distributed with this work for additional information\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m \n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# flake8: noqa\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/web_ing_data/ia-portfolio/venv/lib/python3.12/site-packages/pyarrow/parquet/core.py:32\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpa\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_parquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_parquet\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     35\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe pyarrow installation is not built with support \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     36\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfor the Parquet file format (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(exc)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     37\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:645\u001b[39m, in \u001b[36mparent\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# === CARGAR DATOS DE MÃšLTIPLES FUENTES ===\n",
    "\n",
    "# 1. Cargar datos de viajes desde Parquet (Dataset oficial completo NYC)\n",
    "print(\"Cargando datos oficiales de NYC Taxi (dataset completo)...\")\n",
    "trips_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\"\n",
    "\n",
    "# Cargar dataset oficial (~3M registros de enero 2023)\n",
    "trips = pd.read_parquet(trips_url)  # funciÃ³n para leer archivos .parquet (mÃ¡s eficiente que CSV)\n",
    "\n",
    "print(f\"   Viajes cargados: {trips.shape[0]:,} filas, {trips.shape[1]} columnas\")\n",
    "print(f\"   Columnas: {list(trips.columns)}\")\n",
    "print(f\"   PerÃ­odo: {trips['tpep_pickup_datetime'].min()} a {trips['tpep_pickup_datetime'].max()}\")\n",
    "print(f\"   TamaÃ±o en memoria: {trips.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# 2. Cargar datos de zonas desde CSV (Dataset oficial completo)\n",
    "print(\"\\nCargando datos oficiales de zonas NYC...\")\n",
    "zones_url = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv\"\n",
    "zones = pd.read_csv(zones_url)  # funciÃ³n estÃ¡ndar para archivos CSV\n",
    "\n",
    "print(f\"   Zonas cargadas: {zones.shape[0]} filas, {zones.shape[1]} columnas\")\n",
    "print(f\"   Columnas: {list(zones.columns)}\")\n",
    "print(f\"   Boroughs Ãºnicos: {zones['Borough'].unique()}\")\n",
    "\n",
    "# 3. Cargar calendario de eventos desde JSON \n",
    "print(\"\\nCargando datos de calendario de eventos...\")\n",
    "calendar_url = \"https://juanfkurucz.com/ucu-id/ut1/data/calendar.json\"\n",
    "calendar = pd.read_json(calendar_url)  # funciÃ³n para archivos JSON\n",
    "calendar['date'] = pd.to_datetime(calendar['date']).dt.date  # convertir strings a fechas, luego extraer solo la fecha\n",
    "\n",
    "print(f\"   Eventos calendario: {calendar.shape[0]} filas\")\n",
    "print(f\"   Columnas: {list(calendar.columns)}\")\n",
    "\n",
    "# 4. Mostrar primeras filas de cada dataset\n",
    "print(\"\\nVISTA PREVIA DE DATOS:\")\n",
    "print(\"\\n--- TRIPS ---\")\n",
    "print(trips.head())  # mÃ©todo para mostrar primeras filas de un DataFrame\n",
    "print(\"\\n--- ZONES ---\")\n",
    "print(zones.head())  # mismo mÃ©todo para ver estructura de datos\n",
    "print(\"\\n--- CALENDAR ---\")\n",
    "print(calendar.head())  # revisar formato de los eventos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1162ea54",
   "metadata": {},
   "source": [
    "### ðŸ§¹ Paso 3: NormalizaciÃ³n de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ff3de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === NORMALIZAR Y PREPARAR DATOS PARA JOINS ===\n",
    "\n",
    "# 1. Estandarizar nombres de columnas\n",
    "print(\"Normalizando nombres de columnas...\")\n",
    "trips.columns = trips.columns.str.lower() # convertir todas las columnas a minÃºsculas\n",
    "zones.columns = zones.columns.str._______()  # misma transformaciÃ³n para consistencia\n",
    "\n",
    "print(f\"   Trips columnas: {list(trips.columns)}\")\n",
    "print(f\"   Zones columnas: {list(zones.columns)}\")\n",
    "\n",
    "# 2. Crear columna de fecha para el join con calendario\n",
    "trips['pickup_date'] = trips['_______'].dt.date  # extraer solo la fecha (sin hora) de la columna datetime\n",
    "\n",
    "print(f\"   Columna pickup_date creada\")\n",
    "print(f\"   Rango de fechas: {trips['pickup_date'].min()} a {trips['pickup_date'].max()}\")\n",
    "\n",
    "# 3. Verificar tipos de datos para joins\n",
    "print(\"\\nVERIFICACIÃ“N DE TIPOS PARA JOINS:\")\n",
    "print(f\"   trips['pulocationid'] tipo: {trips['pulocationid'].dtype}\")\n",
    "print(f\"   zones['locationid'] tipo: {zones['locationid'].dtype}\")\n",
    "print(f\"   trips['pickup_date'] tipo: {type(trips['pickup_date'].iloc[0])}\")\n",
    "print(f\"   calendar['date'] tipo: {type(calendar['date'].iloc[0])}\")\n",
    "\n",
    "# 4. OptimizaciÃ³n para datasets grandes (~3M registros)\n",
    "print(\"\\nOPTIMIZACIÃ“N PARA DATASETS GRANDES:\")\n",
    "initial_memory = trips.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f\"   Memoria inicial: {initial_memory:.1f} MB\")\n",
    "\n",
    "# Optimizar tipos de datos para 3+ millones de registros\n",
    "print(\"   Optimizando tipos de datos para 3M+ registros...\")\n",
    "\n",
    "# Limpiar valores nulos antes de convertir tipos\n",
    "print(\"   Limpiando valores nulos antes de optimizaciÃ³n...\")\n",
    "trips['passenger_count'] = trips['passenger_count']._______()  # mÃ©todo para rellenar valores nulos con un valor especÃ­fico\n",
    "trips = trips.dropna(subset=['pulocationid', 'dolocationid'])  # eliminar filas crÃ­ticas sin ubicaciÃ³n (necesarias para joins)\n",
    "\n",
    "# Convertir tipos despuÃ©s de limpiar\n",
    "trips['pulocationid'] = trips['pulocationid'].astype('int16')\n",
    "trips['dolocationid'] = trips['dolocationid'].astype('int16') \n",
    "trips['passenger_count'] = trips['passenger_count'].astype('int8')\n",
    "zones['locationid'] = zones['locationid'].astype('int16')\n",
    "\n",
    "print(f\"   Registros despuÃ©s de limpieza: {len(trips):,}\")\n",
    "\n",
    "optimized_memory = trips.memory_usage(deep=True).sum() / 1024**2\n",
    "savings = ((initial_memory - optimized_memory) / initial_memory * 100)\n",
    "\n",
    "print(f\"   Memoria optimizada: {optimized_memory:.1f} MB\")\n",
    "print(f\"   Ahorro de memoria: {savings:.1f}%\")\n",
    "\n",
    "# 5. Revisar datos faltantes antes de joins\n",
    "print(\"\\nDATOS FALTANTES ANTES DE JOINS:\")\n",
    "print(\"Trips (top 5 columnas con mÃ¡s nulos):\")\n",
    "trips_nulls = trips._______().sum().sort_values(ascending=False).head()  # mÃ©todo para detectar valores nulos, sumar y ordenar\n",
    "print(trips_nulls)\n",
    "\n",
    "print(\"\\nZones:\")\n",
    "zones_nulls = zones._______().sum()  # revisar si hay valores faltantes en lookup table\n",
    "print(zones_nulls)\n",
    "\n",
    "print(\"\\nCalendar:\")\n",
    "calendar_nulls = calendar._______().sum()  # verificar integridad del calendario de eventos\n",
    "print(calendar_nulls)\n",
    "\n",
    "# AnÃ¡lisis de calidad de datos\n",
    "print(\"\\nANÃLISIS DE CALIDAD:\")\n",
    "total_trips = len(trips)\n",
    "print(f\"   Total de viajes: {total_trips:,}\")\n",
    "print(f\"   Viajes sin pickup location: {trips['pulocationid'].isna().sum():,}\")\n",
    "print(f\"   Viajes sin dropoff location: {trips['dolocationid'].isna().sum():,}\")\n",
    "print(f\"   Viajes sin passenger_count: {trips['passenger_count'].isna().sum():,}\")\n",
    "\n",
    "# Estrategias de limpieza recomendadas\n",
    "print(\"\\nESTRATEGIAS DE LIMPIEZA:\")\n",
    "print(\"   Ubicaciones nulas: Eliminar (crÃ­tico para joins)\")\n",
    "print(\"   Passenger_count nulos: Rellenar con valor tÃ­pico (1)\")\n",
    "print(\"   Tarifas nulas: Revisar caso por caso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52969dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PRIMER JOIN: TRIPS + ZONES ===\n",
    "\n",
    "# 1. Hacer join de trips con zones para obtener informaciÃ³n geogrÃ¡fica\n",
    "print(\"Realizando join: trips + zones...\")\n",
    "trips_with_zones = trips._______(zones,   # mÃ©todo principal para unir DataFrames\n",
    "                                left_on='_______',   # columna de trips que contiene ID de zona de pickup\n",
    "                                right_on='_______',  # columna de zones que contiene ID correspondiente\n",
    "                                how='_______')       # tipo de join que mantiene todos los trips\n",
    "\n",
    "print(f\"   Registros antes del join: {len(trips)}\")\n",
    "print(f\"   Registros despuÃ©s del join: {len(trips_with_zones)}\")\n",
    "print(f\"   Nuevas columnas aÃ±adidas: {[col for col in trips_with_zones.columns if col not in trips.columns]}\")\n",
    "\n",
    "# 2. Verificar el resultado del join\n",
    "print(\"\\nVERIFICACIÃ“N DEL JOIN:\")\n",
    "print(\"Conteo por Borough:\")\n",
    "print(trips_with_zones['borough'].value_counts())\n",
    "\n",
    "# 3. Verificar si hay valores nulos despuÃ©s del join\n",
    "null_after_join = trips_with_zones['borough']._______().sum()  # contar nulos en columna borough\n",
    "print(f\"\\nViajes sin borough asignado: {null_after_join}\")\n",
    "\n",
    "if null_after_join > 0:\n",
    "    print(\"   Algunos viajes no encontraron su zona correspondiente\")\n",
    "    print(\"   LocationIDs problemÃ¡ticos:\")\n",
    "    problematic_ids = trips_with_zones[trips_with_zones['borough']._______()]['PULocationID'].unique()  # filtrar filas con nulos\n",
    "    print(f\"   {problematic_ids}\")\n",
    "\n",
    "# 4. Mostrar muestra del resultado\n",
    "print(\"\\nMUESTRA DEL DATASET INTEGRADO:\")\n",
    "print(trips_with_zones[['PULocationID', 'borough', 'zone', 'trip_distance', 'total_amount']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee20ea91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SEGUNDO JOIN: TRIPS_ZONES + CALENDAR ===\n",
    "\n",
    "# 1. Hacer join con datos de calendario\n",
    "print(\"Realizando join: trips_zones + calendar...\")\n",
    "trips_complete = trips_with_zones.merge(calendar,   # mismo mÃ©todo de join que antes\n",
    "                                         left_on='_______',   # columna de fecha que creamos en trips\n",
    "                                         right_on='_______',  # columna de fecha en calendar\n",
    "                                         how='_______')       # tipo que mantiene todos los trips aunque no haya evento especial\n",
    "\n",
    "print(f\"   Registros antes del join: {len(trips_with_zones)}\")\n",
    "print(f\"   Registros despuÃ©s del join: {len(trips_complete)}\")\n",
    "\n",
    "# 2. Crear flag de evento especial\n",
    "trips_complete['is_special_day'] = trips_complete['special']._______('False')  # mÃ©todo para rellenar nulos con valor por defecto\n",
    "\n",
    "print(\"\\nDISTRIBUCIÃ“N DE DÃAS ESPECIALES:\")\n",
    "print(trips_complete['is_special_day'].value_counts())\n",
    "print(\"\\nEjemplos de eventos especiales:\")\n",
    "special_days = trips_complete[trips_complete['is_special_day'] == True]\n",
    "if len(special_days) > 0:\n",
    "    print(special_days[['pickup_date', 'special', 'borough']].drop_duplicates())\n",
    "else:\n",
    "    print(\"   No hay eventos especiales en este perÃ­odo\")\n",
    "\n",
    "# 3. Mostrar dataset final integrado\n",
    "print(\"\\nDATASET FINAL INTEGRADO:\")\n",
    "print(f\"   Total registros: {len(trips_complete)}\")\n",
    "print(f\"   Total columnas: {len(trips_complete.columns)}\")\n",
    "print(f\"   Columnas principales: {['borough', 'zone', 'is_special_day', 'trip_distance', 'total_amount']}\")\n",
    "\n",
    "# 4. Verificar integridad de los datos finales\n",
    "print(\"\\nVERIFICACIÃ“N FINAL:\")\n",
    "print(\"Datos faltantes por columna clave:\")\n",
    "key_columns = ['borough', 'zone', 'trip_distance', 'total_amount', 'is_special_day']\n",
    "for col in key_columns:\n",
    "    missing = trips_complete[col].isna().sum()  # verificar nulos en cada columna clave final\n",
    "    print(f\"   {col}: {missing} nulos\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
